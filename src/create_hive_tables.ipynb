{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE GOAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# import data types to cast data and define schema\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, \\\n",
    "                                DecimalType, FloatType\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"hdfs://namenode/sql/metadata/hive\")\\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()\n",
    "# .config(\"spark.sql.warehouse.dir\", \"hdfs://namenode/sql/metadata/hive\") \\\n",
    "\n",
    "hive_folder = \"/Users/vk/Documents/Python/holmes_moriarty_sql/src/hive_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%rm -rf metastore_db/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select database to use and create tables in\n",
    "\n",
    "spark.sql(\"use default\").show(10, False)  # 'default' is a pre-created database where we can create tables\n",
    "# (we could have skipped this statement but it makes it more explicity which database we use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|default  |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# we could also create our tables in our own custom database using:\n",
    "# spark.sql(\"create database moriarty_db\")\n",
    "# spark.sql(\"use moriarty_db\")\n",
    "spark.sql(\"show tables\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tables = [i[\"tableName\"] for i in spark.sql(\"show tables\").collect()]\n",
    "# tables\n",
    "# for table in tables:\n",
    "#     spark.sql(\"drop table if exists {}\".format(table))\n",
    "    \n",
    "# spark.sql(\"show tables\").show(10, False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crime_type_profit_France.txt          criminals_Germany.csv\r\n",
      "crime_type_profit_Germany.txt         criminals_Netherlands.csv\r\n",
      "crime_type_profit_Netherlands.txt     criminals_United Kingdom.csv\r\n",
      "crime_type_profit_United Kingdom.txt  id_dates.csv\r\n",
      "criminals_France.csv\r\n"
     ]
    }
   ],
   "source": [
    "%ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criminals_country_template=\"\"\"CREATE EXTERNAL TABLE IF NOT EXISTS criminals_{} (\n",
    " id int,\n",
    " name string,\n",
    " alias string,\n",
    " latitude float,\n",
    " longitude float,\n",
    " country string\n",
    "    )\n",
    "STORED AS ORC\n",
    "LOCATION '{}'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_schema(df, schema):\n",
    "    \"\"\"\"\"\"\n",
    "    # apply schema\n",
    "    df = spark.createDataFrame(df.rdd, schema=schema)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country: United Kingdom, rows: 306\n",
      "Country: Germany, rows: 264\n",
      "Country: Netherlands, rows: 250\n",
      "Country: France, rows: 349\n",
      "+--------+------------------------+-----------+\n",
      "|database|tableName               |isTemporary|\n",
      "+--------+------------------------+-----------+\n",
      "|default |criminals_france        |false      |\n",
      "|default |criminals_germany       |false      |\n",
      "|default |criminals_netherlands   |false      |\n",
      "|default |criminals_united_kingdom|false      |\n",
      "+--------+------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the data\n",
    "\n",
    "def rename_cols(df, new_col_names):\n",
    "    \"\"\"\"\"\"\n",
    "    for col, new_col in zip(df.columns, new_col_names):\n",
    "        df = df.withColumnRenamed(col, new_col)\n",
    "        \n",
    "    return df\n",
    "\n",
    "#explore the dataframes: column names, shapes and combine into a single dataframe\n",
    "country_list = [\"United Kingdom\", \"Germany\", \"Netherlands\", \"France\"]\n",
    "\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"alias\", StringType(), True),\n",
    "    StructField(\"latitude\", FloatType(), True),\n",
    "    StructField(\"longitude\", FloatType(), True),\n",
    "    StructField(\"country\", StringType(), True)\n",
    "])\n",
    "\n",
    "for country_ in country_list:\n",
    "    file_name = \"./data/criminals_{}.csv\".format(country_)\n",
    "    df = spark.read.csv(file_name, header=True, inferSchema=True)\n",
    "    print(\"Country: {}, rows: {}\".format(country_, df.count()))\n",
    "    new_col_names = [\"id\", \"name\", \"alias\", \"latitude\", \"longitude\"]\n",
    "    df = rename_cols(df, new_col_names)\n",
    "    df = df.withColumn('country', F.lit(country_))\n",
    "    country_ = \"_\".join(country_.split()).lower()  # 'United Kingdom' as space in it and thus is an illigal table name\n",
    "    df = apply_schema(df, schema)\n",
    "    location = os.path.join(hive_folder, 'criminals_{}'.format(country_))\n",
    "    df.persist()\n",
    "    df.write.orc(location, mode=\"overwrite\")\n",
    "    df.unpersist()\n",
    "\n",
    "    \n",
    "    # register table\n",
    "    spark.sql(criminals_country_template.format(country_, location))\n",
    "    \n",
    "spark.sql(\"show tables\").show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------+-----+--------+---------+--------------+\n",
      "|id |name                    |alias|latitude|longitude|country       |\n",
      "+---+------------------------+-----+--------+---------+--------------+\n",
      "|0  |Ms. Diane Barnett       |null |51.3327 |-0.0328  |United Kingdom|\n",
      "|1  |Elizabeth McDonald      |null |51.3732 |-0.0396  |United Kingdom|\n",
      "|2  |Jacqueline Martin-Winter|null |51.3536 |-0.223   |United Kingdom|\n",
      "+---+------------------------+-----+--------+---------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from criminals_united_kingdom\").show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "criminals_unioned count: 1169\n",
      "+---+--------------------+-----+--------+---------+-------+\n",
      "| id|                name|alias|latitude|longitude|country|\n",
      "+---+--------------------+-----+--------+---------+-------+\n",
      "|  0|Henriette Thomas ...| null| 48.9072|   2.2521| France|\n",
      "|190|Inès Martins de l...| null| 49.0029|    2.251| France|\n",
      "|215|         Isaac Labbe| null| 48.8873|   2.3601| France|\n",
      "+---+--------------------+-----+--------+---------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_criminals_combined = spark.sql(\"\"\"select * from criminals_france\n",
    "          union\n",
    "          select * from criminals_germany\n",
    "          union\n",
    "          select * from criminals_netherlands\n",
    "          union\n",
    "          select * from criminals_united_kingdom\"\"\")\n",
    "\n",
    "print(\"criminals_unioned count: {}\".format(df_criminals_combined.count()))\n",
    "\n",
    "df_criminals_combined.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+\n",
      "|country       |count(1)|\n",
      "+--------------+--------+\n",
      "|Germany       |264     |\n",
      "|France        |349     |\n",
      "|United Kingdom|306     |\n",
      "|Netherlands   |250     |\n",
      "+--------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_criminals_combined.groupBy(\"country\").agg(F.count(\"*\")).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register temp table\n",
    "df_criminals_combined.registerTempTable(\"criminals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|country       |\n",
      "+--------------+\n",
      "|Germany       |\n",
      "|France        |\n",
      "|United Kingdom|\n",
      "|Netherlands   |\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "criminals_df = spark.sql(\"select distinct country from criminals\")\n",
    "# print(\"Table (read-in) count: {}\".format(criminals_df.count()))\n",
    "criminals_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+--------------------+\n",
      "|country       |avg_lat           |avg_lon             |\n",
      "+--------------+------------------+--------------------+\n",
      "|France        |48.86060943166301 |2.364566761989648   |\n",
      "|Germany       |50.097135254831024|8.678964380061988   |\n",
      "|Netherlands   |52.37530560302734 |4.900951610565185   |\n",
      "|United Kingdom|51.50456336276983 |-0.12400588218790493|\n",
      "+--------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculate mean latitude and longitude to identify the major financial centers (cities)\n",
    "# (copy and paste the lat, lon values into Google Maps)\n",
    "# dataframe.filter(df['salary'] > 100000).agg({\"age\": \"avg\"})\n",
    "\n",
    "spark.sql(\"\"\"select country, \n",
    "                    AVG(latitude) as avg_lat, \n",
    "                    AVG(longitude) as avg_lon\n",
    "                    from criminals\n",
    "                    group by country\n",
    "                    order by country\"\"\").show(10, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+-------+\n",
      "|country       |avg_lat|avg_lon|\n",
      "+--------------+-------+-------+\n",
      "|France        |48.8606|2.3646 |\n",
      "|Germany       |50.0971|8.679  |\n",
      "|Netherlands   |52.3753|4.901  |\n",
      "|United Kingdom|51.5046|-0.124 |\n",
      "+--------------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select country,\n",
    "                    ROUND(AVG(latitude), 4) as avg_lat,\n",
    "                    ROUND(AVG(longitude), 4) as avg_lon\n",
    "                from criminals\n",
    "                 group by country\n",
    "                 order by country\"\"\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'United Kingdom': 'London',\n",
       " 'Germany': 'Frankfurt',\n",
       " 'Netherlands': 'Amsterdam',\n",
       " 'France': 'Paris'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add the city name to the df\n",
    "\n",
    "#it can be done using a series of if/else statements, such as 'if country_ == 'France': city = 'Paris', etc. OR\n",
    "# using a dictionary as below:\n",
    "country_city_dict = {\"United Kingdom\": \"London\", \"Germany\": \"Frankfurt\", \"Netherlands\": \"Amsterdam\", \"France\": \"Paris\"}\n",
    "country_city_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------------+-----+--------+---------+-----------+---------+\n",
      "|id |name                     |alias|latitude|longitude|country    |city     |\n",
      "+---+-------------------------+-----+--------+---------+-----------+---------+\n",
      "|0  |Henriette Thomas du Peron|null |48.9072 |2.2521   |France     |Paris    |\n",
      "|190|Inès Martins de la Morel |null |49.0029 |2.251    |France     |Paris    |\n",
      "|215|Isaac Labbe              |null |48.8873 |2.3601   |France     |Paris    |\n",
      "|244|Aimée-Margot Martins     |null |48.885  |2.5288   |France     |Paris    |\n",
      "|297|Olivier Lopez            |Flako|48.616  |2.1133   |France     |Paris    |\n",
      "|326|Philippine Traore        |null |49.0171 |2.4338   |France     |Paris    |\n",
      "|23 |Dipl.-Ing. Dennis Hesse  |null |50.0266 |8.6771   |Germany    |Frankfurt|\n",
      "|202|Ali Dowerg               |null |50.1855 |8.5548   |Germany    |Frankfurt|\n",
      "|243|Tomas Mies               |null |50.0033 |8.9106   |Germany    |Frankfurt|\n",
      "|232|Liam Janssen-Frankhuizen |null |52.5289 |4.8684   |Netherlands|Amsterdam|\n",
      "+---+-------------------------+-----+--------+---------+-----------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select *, \n",
    "                case \n",
    "                    when country = 'United Kingdom' then 'London'\n",
    "                    when country = 'France' then 'Paris'\n",
    "                    when country = 'Germany' then 'Frankfurt'\n",
    "                    when country = 'Netherlands' then 'Amsterdam'\n",
    "                end as city\n",
    "            from criminals\"\"\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------------+-----+-----------+---------+\n",
      "|id |name                     |alias|country    |city     |\n",
      "+---+-------------------------+-----+-----------+---------+\n",
      "|0  |Henriette Thomas du Peron|     |France     |Paris    |\n",
      "|190|Inès Martins de la Morel |     |France     |Paris    |\n",
      "|215|Isaac Labbe              |     |France     |Paris    |\n",
      "|244|Aimée-Margot Martins     |     |France     |Paris    |\n",
      "|297|Olivier Lopez            |Flako|France     |Paris    |\n",
      "|326|Philippine Traore        |     |France     |Paris    |\n",
      "|23 |Dipl.-Ing. Dennis Hesse  |     |Germany    |Frankfurt|\n",
      "|202|Ali Dowerg               |     |Germany    |Frankfurt|\n",
      "|243|Tomas Mies               |     |Germany    |Frankfurt|\n",
      "|232|Liam Janssen-Frankhuizen |     |Netherlands|Amsterdam|\n",
      "+---+-------------------------+-----+-----------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fill Nas with empty string \n",
    "# we'll also assign this new data to a variable name for saving and creating a new table to use later\n",
    "# (note that 'show' method is moved to the spark dataframe)\n",
    "criminals_with_city_df = spark.sql(\"\"\"select id, name,\n",
    "                case\n",
    "                    when alias is null then ''\n",
    "                    else alias\n",
    "                end as alias,\n",
    "                country,\n",
    "                case \n",
    "                    when country = 'United Kingdom' then 'London'\n",
    "                    when country = 'France' then 'Paris'\n",
    "                    when country = 'Germany' then 'Frankfurt'\n",
    "                    when country = 'Netherlands' then 'Amsterdam'\n",
    "                end as city\n",
    "            from criminals\"\"\")\n",
    "\n",
    "criminals_with_city_df.cache()\n",
    "criminals_with_city_df.registerTempTable(\"criminals_with_city\")\n",
    "\n",
    "criminals_with_city_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(country='Germany'),\n",
       " Row(country='France'),\n",
       " Row(country='United Kingdom'),\n",
       " Row(country='Netherlands')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select distinct country from criminals\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(country='Germany'),\n",
       " Row(country='France'),\n",
       " Row(country='United Kingdom'),\n",
       " Row(country='Netherlands')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that the data is readable\n",
    "spark.sql(\"select distinct country from criminals_with_city\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+\n",
      "|country       |count(1)|\n",
      "+--------------+--------+\n",
      "|Germany       |264     |\n",
      "|France        |349     |\n",
      "|United Kingdom|306     |\n",
      "|Netherlands   |250     |\n",
      "+--------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "criminals_with_city_df.groupBy(\"country\").agg(F.count(\"*\")).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------------+-----------+\n",
      "|database|tableName               |isTemporary|\n",
      "+--------+------------------------+-----------+\n",
      "|default |criminals_france        |false      |\n",
      "|default |criminals_germany       |false      |\n",
      "|default |criminals_netherlands   |false      |\n",
      "|default |criminals_united_kingdom|false      |\n",
      "|        |criminals               |true       |\n",
      "|        |criminals_with_city     |true       |\n",
      "+--------+------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "Add crime_type and profit info to criminals. \n",
    "#(merge/join) criminals table with the crime type and profit information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(country='Germany'),\n",
       " Row(country='France'),\n",
       " Row(country='United Kingdom'),\n",
       " Row(country='Netherlands')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select distinct country from criminals\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Great, Watson! \n",
    "- Now we need to know what everyone of those supspects did wrong, that is the crime type, and desirably, how much they profited from it: Moriarty is not a small fish. He is in the category with th largest total sales.\n",
    "\n",
    "- You'll need to add the crime type and the profit from the files to the table you already put together. Be mindful of the file types. I also believe that the separator in these file maybe different from the files you used previously.\n",
    "-Moriarty made one of the top 5 sales last year. He is not stupid for nicknames, I am pretty sure he doesn't have an alias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution (task 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crime_type_profit_France.txt          criminals_Germany.csv\r\n",
      "crime_type_profit_Germany.txt         criminals_Netherlands.csv\r\n",
      "crime_type_profit_Netherlands.txt     criminals_United Kingdom.csv\r\n",
      "crime_type_profit_United Kingdom.txt  id_dates.csv\r\n",
      "criminals_France.csv\r\n"
     ]
    }
   ],
   "source": [
    "%ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------------+-----------+\n",
      "|database|tableName               |isTemporary|\n",
      "+--------+------------------------+-----------+\n",
      "|default |criminals_france        |false      |\n",
      "|default |criminals_germany       |false      |\n",
      "|default |criminals_netherlands   |false      |\n",
      "|default |criminals_united_kingdom|false      |\n",
      "|        |criminals               |true       |\n",
      "|        |criminals_with_city     |true       |\n",
      "+--------+------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country: United Kingdom, rows: 306\n",
      "country_:  united_kingdom\n",
      "location:  /Users/vk/Documents/Python/holmes_moriarty_sql/src/hive_data/crime_type_profit_united_kingdom\n",
      "Country: Germany, rows: 264\n",
      "country_:  germany\n",
      "location:  /Users/vk/Documents/Python/holmes_moriarty_sql/src/hive_data/crime_type_profit_germany\n",
      "Country: Netherlands, rows: 250\n",
      "country_:  netherlands\n",
      "location:  /Users/vk/Documents/Python/holmes_moriarty_sql/src/hive_data/crime_type_profit_netherlands\n",
      "Country: France, rows: 349\n",
      "country_:  france\n",
      "location:  /Users/vk/Documents/Python/holmes_moriarty_sql/src/hive_data/crime_type_profit_france\n",
      "+--------+--------------------------------+-----------+\n",
      "|database|tableName                       |isTemporary|\n",
      "+--------+--------------------------------+-----------+\n",
      "|default |crime_type_profit_france        |false      |\n",
      "|default |crime_type_profit_germany       |false      |\n",
      "|default |crime_type_profit_netherlands   |false      |\n",
      "|default |crime_type_profit_united_kingdom|false      |\n",
      "|default |criminals_france                |false      |\n",
      "|default |criminals_germany               |false      |\n",
      "|default |criminals_netherlands           |false      |\n",
      "|default |criminals_united_kingdom        |false      |\n",
      "|        |criminals                       |true       |\n",
      "|        |criminals_with_city             |true       |\n",
      "+--------+--------------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "create_table_template=\"\"\"CREATE EXTERNAL TABLE IF NOT EXISTS crime_type_profit_{} (\n",
    " name string,\n",
    " crime_type string,\n",
    " profit string,\n",
    " country string\n",
    "    )\n",
    "STORED AS ORC\n",
    "LOCATION '{}'\n",
    "\"\"\"\n",
    "\n",
    "# get the data\n",
    "\n",
    "#explore the dataframes: column names, shapes and combine into a single dataframe\n",
    "country_list = [\"United Kingdom\", \"Germany\", \"Netherlands\", \"France\"]\n",
    "\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"crime_type\", StringType(), True),\n",
    "    StructField(\"profit\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True)\n",
    "])\n",
    "\n",
    "for country_ in country_list:\n",
    "    file_name = \"./data/crime_type_profit_{}.txt\".format(country_)\n",
    "    df = spark.read.csv(file_name, header=True, inferSchema=True, sep=\" \")\n",
    "    print(\"Country: {}, rows: {}\".format(country_, df.count()))\n",
    "\n",
    "    df = df.withColumn('country', F.lit(country_))\n",
    "    country_ = \"_\".join(country_.split()).lower()  # 'United Kingdom' has space in it and thus is an illigal table name\n",
    "    print(\"country_: \", country_)\n",
    "    location = os.path.join(hive_folder, 'crime_type_profit_{}'.format(country_))\n",
    "    print(\"location: \", location)\n",
    "    df = apply_schema(df, schema)\n",
    "    df.persist()\n",
    "    df.write.orc(location, mode=\"overwrite\")\n",
    "    df.unpersist()\n",
    "    \n",
    "    # register table\n",
    "    spark.sql(create_table_template.format(country_, location))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "spark.sql(\"show tables\").show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crime_profit count: 1169\n",
      "+---------------+-------------+------+-----------+\n",
      "|           name|   crime_type|profit|    country|\n",
      "+---------------+-------------+------+-----------+\n",
      "|    Isaac David|        theft|   204|     France|\n",
      "|Nikolaj Kallert|pickpocketing|    47|    Germany|\n",
      "|   Leah Winters|    drug sale|  3510|Netherlands|\n",
      "+---------------+-------------+------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crime_profit = spark.sql(\"\"\"select * from crime_type_profit_france\n",
    "          union\n",
    "          select * from crime_type_profit_germany\n",
    "          union\n",
    "          select * from crime_type_profit_netherlands\n",
    "          union\n",
    "          select * from crime_type_profit_united_kingdom\"\"\")\n",
    "\n",
    "print(\"crime_profit count: {}\".format(crime_profit.count()))\n",
    "\n",
    "crime_profit.registerTempTable(\"crime_profit\")\n",
    "\n",
    "crime_profit.show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+\n",
      "|country       |count(1)|\n",
      "+--------------+--------+\n",
      "|Germany       |264     |\n",
      "|France        |349     |\n",
      "|United Kingdom|306     |\n",
      "|Netherlands   |250     |\n",
      "+--------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crime_profit.groupBy(\"country\").agg(F.count(\"*\")).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'criminals_with_city' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-99090d18d344>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# print(criminals.select(\"country\").distinct().collect())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcriminals_with_city\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"country\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistinct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'criminals_with_city' is not defined"
     ]
    }
   ],
   "source": [
    "# print(criminals.select(\"country\").distinct().collect())\n",
    "print(criminals_with_city.select(\"country\").distinct().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criminals_with_city.select(\"country\").distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"select  a.id, a.name, a.alias, b.crime_type, b.profit, b.country, a.city\n",
    "            from criminals_with_city a\n",
    "            left join crime_profit b\n",
    "                on a.name = b.name and a.country = b.country\"\"\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"select count(*) as row_count from (\n",
    "                select  a.id, a.name, a.alias, b.crime_type, b.profit, b.country\n",
    "                from criminals a\n",
    "                left join crime_profit b\n",
    "                    on a.name = b.name and a.country = b.country)\"\"\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order by profit (descending) and \n",
    "# cast profit as int (to keep the same with pyspark notebook; not necessary here)\n",
    "spark.sql(\"\"\"select  a.id, a.name, a.alias, b.crime_type, CAST(b.profit AS INT), b.country\n",
    "            from criminals a\n",
    "            left join crime_profit b\n",
    "                on a.name = b.name and a.country = b.country\n",
    "            order by profit DESC\"\"\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the most profitable crime type\n",
    "\n",
    "spark.sql(\"\"\"select crime_type, sum(profit) as total_profit\n",
    "            from criminals a\n",
    "            left join crime_profit b\n",
    "                on a.name = b.name and a.country = b.country\n",
    "            group by crime_type\n",
    "            order by total_profit DESC\"\"\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark.sql(\"show tables\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the country where with the highest weapons sales\n",
    "\n",
    "spark.sql(\"\"\"select a.country, sum(profit) as total_profit\n",
    "            from criminals a\n",
    "            left join crime_profit b\n",
    "                on a.name = b.name and a.country = b.country\n",
    "            where crime_type = 'weapons sale'\n",
    "            group by a.country\n",
    "            order by total_profit DESC\n",
    "            \"\"\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data with joined data \n",
    "\n",
    "criminals_with_profit = spark.sql(\"\"\"select  a.id, a.name, a.alias, b.crime_type, b.profit, b.country\n",
    "            from criminals_with_city a\n",
    "            left join crime_profit b\n",
    "                on a.name = b.name and a.country = b.country\"\"\")\n",
    "\n",
    "criminals_with_profit.cache()\n",
    "criminals_with_profit.registerTempTable(\"criminals_with_profit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show tables\").show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from criminals_with_profit\").show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criminals_with_profit.groupBy(\"country\").agg(F.count(\"*\")).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"select name, profit \n",
    "            from criminals_with_profit \n",
    "            where crime_type LIKE 'weapon%' \n",
    "                and country = 'France'\n",
    "            order by profit DESC\n",
    "            \"\"\").show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 3\n",
    "\n",
    "Add date (last deal date) Moriarty does not deal on Sundays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_dates = spark.read.csv(\"./data/id_dates.csv\", header=True, inferSchema=True)\n",
    "print(\"id_dates shape: {}\".format(id_dates.count()))\n",
    "id_dates.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected_with_dates = df_city_profit.join(id_dates, on=[\"id\", \"country\"], how=\"left\")\n",
    "print(df_selected_with_dates.count())\n",
    "df_selected_with_dates.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected_with_dates.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DateType, StringType\n",
    "\n",
    "\n",
    "def weekday(date):\n",
    "    \"\"\" Generate day of the week based on date (as string or as datetime object)\"\"\"\n",
    "    \n",
    "    if isinstance(date, str):\n",
    "        from datetime import datetime\n",
    "        \n",
    "        date = datetime.strptime(date, \"%Y-%m-%d\")  # change the format if necessary\n",
    "        \n",
    "    return date.strftime(\"%A\")\n",
    "\n",
    "\n",
    "weekday_udf = udf(weekday, StringType())\n",
    "\n",
    "# conversion to DateType is not necessary as it is handled inside the function\n",
    "# here it is offered as an example of re-casting\n",
    "df_selected_with_dates = df_selected_with_dates.withColumn(\"date\", F.col(\"date\").cast(DateType()))\n",
    "\n",
    "df_selected_with_dates = df_selected_with_dates.withColumn(\"weekdate\", weekday_udf(\"date\").alias(\"weekday\"))\n",
    "df_selected_with_dates.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_type_big_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top 5 salesmen in the selected country\n",
    "df_final = df_selected_with_dates.where(\"\"\"country = '{}' \n",
    "                                            and alias = '' \n",
    "                                            and crime_type = '{}'\n",
    "                                            and weekday != 'Sunday'\n",
    "                                       \"\"\".format(top_country, crime_type_big_sales))\n",
    "\n",
    "df_final.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moriarty_name =  df_final.select(\"name\").collect()[0][0]\n",
    "print(\"The name Moriarty is hiding behind: {}\".format(moriarty_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
