{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first part of the notebook creates the tables with the data.\n",
    "# The second part uses pysparkSQL to solve the puzzle.\n",
    "\n",
    "# Just like the 'pyspark_solution' notebook, the notebook uses pyspark as the data retrieving framework \n",
    "# but uses complete Hive statements to select the data.\n",
    "\n",
    "# The statements, taken separately, could be used to retrieve the data from permanent tables using pure Hive.\n",
    "\n",
    "# To be able to read and write Hive statements which borrow significant similarity with SQL statements \n",
    "# is a useful skill in the area of data science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SparkSession to use pysparkSQL\n",
    "\n",
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# import data types to cast data and define schema\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, \\\n",
    "                                DecimalType, FloatType\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"hdfs://namenode/sql/metadata/hive\")\\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()\n",
    "\n",
    "hive_folder = \"/Users/vk/Documents/Python/holmes_moriarty_sql/src/hive_data\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# CREATE the main 'criminals' table with the criminals from each country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%rm -rf metastore_db/  # removes any tables that might be presetn from previous runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select database to use and create tables in\n",
    "# 'default' is a pre-created database where we can create tables\n",
    "spark.sql(\"use default\").show(10, False)  \n",
    "# (we could have skipped this statement but it makes it more explicit which database we use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show databases\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initially there are no tables \n",
    "spark.sql(\"show tables\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a template that till be used to register the 'criminals' table for each country\n",
    "criminals_country_template=\"\"\"CREATE EXTERNAL TABLE IF NOT EXISTS criminals_{} (\n",
    " id int,\n",
    " name string,\n",
    " alias string,\n",
    " latitude float,\n",
    " longitude float,\n",
    " country string\n",
    "    )\n",
    "STORED AS ORC\n",
    "LOCATION '{}'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_schema(df, schema):\n",
    "    \"\"\"Define data types for data frame columns\"\"\"\n",
    "    \n",
    "    df = spark.createDataFrame(df.rdd, schema=schema)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def rename_cols(df, new_col_names):\n",
    "    \"\"\"\"\"\"\n",
    "    for col, new_col in zip(df.columns, new_col_names):\n",
    "        df = df.withColumnRenamed(col, new_col)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data\n",
    "\n",
    "#explore the dataframes: column names, shapes and combine into a single dataframe\n",
    "country_list = [\"United Kingdom\", \"Germany\", \"Netherlands\", \"France\"]\n",
    "\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"alias\", StringType(), True),\n",
    "    StructField(\"latitude\", FloatType(), True),\n",
    "    StructField(\"longitude\", FloatType(), True),\n",
    "    StructField(\"country\", StringType(), True)\n",
    "])\n",
    "\n",
    "for country_ in country_list:\n",
    "    file_name = \"./data/criminals_{}.csv\".format(country_)\n",
    "    df = spark.read.csv(file_name, header=True, inferSchema=True)\n",
    "    print(\"Country: {}, rows: {}\".format(country_, df.count()))\n",
    "    new_col_names = [\"id\", \"name\", \"alias\", \"latitude\", \"longitude\"]\n",
    "    df = rename_cols(df, new_col_names)\n",
    "    df = df.withColumn('country', F.lit(country_))\n",
    "    country_ = \"_\".join(country_.split()).lower()  # 'United Kingdom' as space in it and thus is an illigal table name\n",
    "    df = apply_schema(df, schema)\n",
    "    location = os.path.join(hive_folder, 'criminals_{}'.format(country_))\n",
    "    df.persist()\n",
    "    df.write.orc(location, mode=\"overwrite\")\n",
    "    df.unpersist()\n",
    "    \n",
    "    # register external table\n",
    "    spark.sql(criminals_country_template.format(country_, location))\n",
    "    \n",
    "spark.sql(\"show tables\").show(20, False) # isTem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the table\n",
    "spark.sql(\"select * from criminals_united_kingdom\").show(3, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create crime and profit tables for each country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just as \n",
    "\n",
    "create_table_template=\"\"\"CREATE EXTERNAL TABLE IF NOT EXISTS crime_type_profit_{} (\n",
    " name string,\n",
    " crime_type string,\n",
    " profit string,\n",
    " country string\n",
    "    )\n",
    "STORED AS ORC\n",
    "LOCATION '{}'\n",
    "\"\"\"\n",
    "\n",
    "# get the data\n",
    "\n",
    "#explore the dataframes: column names, shapes and combine into a single dataframe\n",
    "country_list = [\"United Kingdom\", \"Germany\", \"Netherlands\", \"France\"]\n",
    "\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"crime_type\", StringType(), True),\n",
    "    StructField(\"profit\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True)\n",
    "])\n",
    "\n",
    "for country_ in country_list:\n",
    "    file_name = \"./data/crime_type_profit_{}.txt\".format(country_)\n",
    "    df = spark.read.csv(file_name, header=True, inferSchema=True, sep=\" \")\n",
    "    print(\"Country: {}, rows: {}\".format(country_, df.count()))\n",
    "\n",
    "    df = df.withColumn('country', F.lit(country_))\n",
    "    country_ = \"_\".join(country_.split()).lower()  # 'United Kingdom' has space in it and thus is an illigal table name\n",
    "    print(\"country_: \", country_)\n",
    "    location = os.path.join(hive_folder, 'crime_type_profit_{}'.format(country_))\n",
    "    print(\"location: \", location)\n",
    "    df = apply_schema(df, schema)\n",
    "    df.persist()\n",
    "    df.write.orc(location, mode=\"overwrite\")\n",
    "    df.unpersist()\n",
    "    \n",
    "    # register table\n",
    "    spark.sql(create_table_template.format(country_, location))\n",
    "    \n",
    "spark.sql(\"show tables\").show(20, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the table with crime dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "id_dates = spark.read.csv(\"./data/id_dates.csv\", header=True, inferSchema=True)\n",
    "print(\"id_dates shape: {}\".format(id_dates.count()))\n",
    "id_dates.registerTempTable(\"id_dates\")\n",
    "spark.sql(\"select count(distinct country) from id_dates\").show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check available tables ones more\n",
    "spark.sql(\"show tables\").show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE GOAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to use the intel (data in the supplied files) about Europe's criminals from the police, Interpol, and undercover agents  to identify the name behind which Moriarty is hiding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1 \n",
    "\n",
    "-Watson, just like our great-grandfathers we are after Moriarty again.\n",
    "\n",
    "We need to catch him... or maybe it is a her. All we know is that someone is masterminding unlawful activities and planning something terrible. The Interpol agents, with the help of my informants, collected information that should provide us the clues to determine the name Moriarty is hiding behind, and arrest him.\n",
    "\n",
    "-The data is in the csv and text files and contains info on the criminal activity in the last year as well as high-profile and suspicious sales. They were sent over by our collegues from the neighboring countries: France, Germany, Netherlands, and our own MI-6 in the United Kingdom.\n",
    "\n",
    "-The first task would be to combine the data into one table. I requested the information on the name, alias, and the location of the last known whereabouts of the criminals, as latitude and longitude, but since the data comes from all around the Europe the columns names may differ between files.\n",
    "\n",
    "-I am thinking that adding the country of origin to the data might be helpful in our future analysis.\n",
    "\n",
    "-Lastly, from my correspondence with our undercover agents, all the activity seems to be happening around major financial centers. If those centers are not in the data, I suppose you can extract the city names using the latitude and logitude. And a map of course, unless your knowledge of Europe's geography is excepitonal.\n",
    "\n",
    "Data tasks outline:\n",
    "1. Read the data from the files (named 'criminals_' plus country name) into separate dataframes and add the country name as 'country' column.\n",
    "2. Identify the city around which the criminals operate and add it to the dataframe as 'city' column.\n",
    "3. Concatenate the dataframes into a single dataframe with the four original columns renamed to: [name, alias, latitude, longitude]\n",
    "4. Fill NAs in aliases with an empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use complete 'select' SQL statements as part of the pysparkSQL. A general example:\n",
    "temp_df = spark.sql(\"\"\"select * from criminals_france \n",
    "                            where name like '%a'\n",
    "                            order by name\"\"\")\n",
    "temp_df.registerTempTable(\"temp_df\")\n",
    "temp_df.show(2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 \n",
    "Add crime_type and profit info to criminals. \n",
    "#(merge/join) criminals table with the crime type and profit information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
   "\n",
   "-Great, Watson! \n",
   "\n",
   "-Now we need to know what each one did wrong, that is the crime type, and desirably, how much they profited from it: Moriarty is no small fish. He is in the category with the largest total sales.\n",
   "\n",
   "-You'll need to add the crime type and the profit from the files to the table you already put together. Be mindful of the file types. I also believe that the separator in these file maybe different from the files you used previously. \n",
   "\n",
   "-Moriarty made one of the top 5 sales last year. He is not stupid for nicknames, I am pretty sure he doesn't have an alias.\n",
   "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution (task 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 3\n",
    "\n",
    "-Watson, I think we got the last piece of the puzzle!\n",
    "\n",
    "-I just learnt that Moriarty doesn't do his dealings on Sunday.\n",
    "\n",
    "-That means that the top seller (in the country with the top sale in the last year) who didn't sell on a Sunday and who doesn't have an alias will be him.\n",
    "\n",
    "-All we have to do now is add the date information I just got and determine the weekday for that date. We already know the rest.\n",
    "\n",
    "-And we'll send Lestrade right after him!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# moriarty_name = ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
