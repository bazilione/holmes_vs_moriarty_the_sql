{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE GOAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to take Watson's role and using the intel (the data in the supplied files) from the police, Interpol, and undercover agents about Europe's criminals to identify the name behind which Moriarty is hiding. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1\n",
    "-Watson, just like our grand-grand-fathers we are again after Moriarty. \n",
    "\n",
    "We need to catch him. H-mmm... I need to be careful here - maybe it is not him, maybe it is her. All we know is \n",
    "that someone is masterminding unlawful activities and planning something bad. The Interpol agents, with the help of my boys, collected information that should provide us the clues to determine the name Moriarty's is hiding brhind and arrest him.\n",
    "\n",
    "-I have a number of .csv and .txt files about criminal activity and high-profile suspicious sales that were sent over from our neighbors: France, Germany, Netherlands, and our own MI-6 in the United Kingdom.\n",
    "\n",
    "So, the first task would be to combine the data into one table. I requested info on the name, alias, and the location of the last known whereabouts, as latitude and longitude, but since the data comes from all around the Europe they might have named the columns differently.\n",
    "\n",
    "I am thinking that adding the country to the data might be helpful in our future analysis.\n",
    "\n",
    "Lastly, from my correspondence with our undercover agents, all the activity seems to be happening around major financial centers. If the city names are not in the data, I suppose you can extract it based on the latitude and logitude. Mmmm... And a map of course, unless your knowledge of Europe's geography is excepitonal. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Text:\n",
    "Tasks:\n",
    "1. Read in data from the files into a separate dataframe and add the country name ('country' column).\n",
    "2. Identify the city around which the criminals operate. Add it to the dataframe ('city' column).\n",
    "3. Concatenate dfs into a single dataframe with the four original columns renamed to: [name, alias, latitude, longitude]\n",
    "4. Fill NAs in aliases with an empty string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Holmes_Moriarty_SQL\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample one of the csvs\n",
    "country_ = \"France\"\n",
    "file_name = \"./data/criminals_{}.csv\".format(country_)\n",
    "df_country = spark.read.csv(file_name, header=True, inferSchema=True)\n",
    "print(df_country.columns)\n",
    "df_country.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_cols(df, new_col_names):\n",
    "    \"\"\"\"\"\"\n",
    "    for col, new_col in zip(df.columns, new_col_names):\n",
    "        df = df.withColumnRenamed(col, new_col)\n",
    "        \n",
    "    return df\n",
    "\n",
    "#explore the dataframes: column names, shapes and combine into a single dataframe\n",
    "country_list = [\"United Kingdom\", \"Germany\", \"Netherlands\", \"France\"]\n",
    "dfs_dict = {}\n",
    "for country_ in country_list:\n",
    "    file_name = \"./data/criminals_{}.csv\".format(country_)\n",
    "    df = spark.read.csv(file_name, header=True, inferSchema=True)\n",
    "    print(\"Country: {}, rows: {}\".format(country_, df.count()))\n",
    "    new_col_names = [\"id\", \"name\", \"alias\", \"latitude\", \"longitude\"]\n",
    "    df = rename_cols(df, new_col_names)\n",
    "    df = df.withColumn('country', F.lit(country_))\n",
    "    dfs_dict[country_] = df  # add data frame to the dict for a future union\n",
    "print(\"Len dfs_dict: {}\".format(len(dfs_dict)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(dfs_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://datascience.stackexchange.com/questions/11356/merging-multiple-data-frames-row-wise-in-pyspark\n",
    "from functools import reduce  # For Python 3.x\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def unionAll(dfs):\n",
    "    return reduce(DataFrame.unionAll, dfs)\n",
    "\n",
    "df_criminals_combined = unionAll(list(dfs_dict.values()))\n",
    "print(\"Rows in combined df: {}\".format(df_criminals_combined.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alternatively, a chaining union one-by-one\n",
    "dfs_list = list(dfs_dict.values())\n",
    "df_combined = dfs_list[0].union(dfs_list[1]).union(dfs_list[2]).union(dfs_list[3])\n",
    "print(\"Rows in combined df: {}\".format(df_criminals_combined.count()))\n",
    "df_criminals_combined.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate mean latitude and longitude to identify the major financial centers (cities)\n",
    "# (copy and paste the lat, lon values into Google Maps)\n",
    "# dataframe.filter(df['salary'] > 100000).agg({\"age\": \"avg\"})\n",
    "for country_ in country_list:\n",
    "    country_df = df_criminals_combined.where(\"country = '{}'\".format(country_))\n",
    "    lat = round(country_df.agg({\"latitude\": \"avg\"}).collect()[0][0], 4)\n",
    "    lon = round(country_df.agg({\"longitude\": \"avg\"}).collect()[0][0], 4)\n",
    "    print(\"Country: {}, (lat, lon): {}, {}\".format(country_, lat, lon))\n",
    "    print(40 * \"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the city name to the df\n",
    "\n",
    "#it can be done using a series of if/else statements, such as 'if country_ == 'France': city = 'Paris', etc. OR\n",
    "# using a dictionary as below:\n",
    "country_city_dict = {\"United Kingdom\": \"London\", \"Germany\": \"Frankfurt\", \"Netherlands\": \"Amsterdam\", \"France\": \"Paris\"}\n",
    "country_city_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "df_with_city = df_criminals_combined.withColumn('city', \\\n",
    "                                                 when(F.col('country')=='United Kingdom', 'London').\\\n",
    "                                                 when(F.col('country')=='France', 'Paris').\\\n",
    "                                                 when(F.col('country')=='Germany', 'Frankfurt').\\\n",
    "                                                 when(F.col('country')=='Netherlands', 'Amsterdam').\\\n",
    "                                                 otherwise(None))\n",
    "df_with_city.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fillna in alias.\n",
    "df_with_city = df_with_city.fillna({\"alias\": \"\"})\n",
    "print(\"Df shape: {}\".format(df_with_city.count()))\n",
    "df_with_city.orderBy(\"name\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "Add crime_type and profit info to criminals. \n",
    "#(merge/join) criminals table with the crime type and profit information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Great, Watson! \n",
    "- Now we need to know what everyone of those supspects did wrong, that is the crime type, and desirably, how much they profited from it: Moriarty is not a small fish. He is in the category with th largest total sales.\n",
    "\n",
    "- You'll need to add the crime type and the profit from the files to the table you already put together. Be mindful of the file types. I also believe that the separator in these file maybe different from the files you used previously.\n",
    "-Moriarty made one of the top 5 sales last year. He is not stupid for nicknames, I am pretty sure he doesn't have an alias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution (task 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"./data/crime_type_profit_France.txt\", header=True, sep=\" \")\n",
    "print(\"Columns: \", list(df.columns))\n",
    "df.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# union(concatenate) files for the latest crime dates\n",
    "\n",
    "country_list = [\"United Kingdom\", \"Germany\", \"Netherlands\", \"France\"]\n",
    "dfs_dict = {}\n",
    "for country_ in country_list:\n",
    "    file_name = \"./data/crime_type_profit_{}.txt\".format(country_)\n",
    "    df = spark.read.csv(file_name, header=True, sep=\" \")\n",
    "    print(\"rows: {}\".format(df.count()))\n",
    "    df = df.withColumn('country', F.lit(country_))\n",
    "    dfs_dict[country_] = df\n",
    "print(\"Len dfs_dict: {}\".format(len(dfs_dict)))\n",
    "\n",
    "#combine all dataframes into one\n",
    "df_crime_type_profit = unionAll(list(dfs_dict.values()))\n",
    "print(list(df_crime_type_profit.columns))\n",
    "\n",
    "df_crime_type_profit.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates \n",
    "df_with_city = df_with_city.drop_duplicates([\"name\"])\n",
    "df_with_city.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join main criminal info with crime type and profit\n",
    "df_city_profit = df_with_city.join(df_crime_type_profit, [\"name\",\"country\"], \"left\")\n",
    "print(\"Df shape: {}\".format(df_city_profit.count()))\n",
    "# print(df_city_profit.columns)\n",
    "df_city_profit.orderBy('profit', ascending = False).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profit column is not sorted properly. possibly the data type is the issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city_profit.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city_profit = df_city_profit.withColumn(\"profit\", F.col(\"profit\").cast(\"int\"))\n",
    "df_city_profit.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's order by profit again...\n",
    "df_city_profit.orderBy('profit', ascending = False).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#investigate crime types and get total sales for each\n",
    "df_by_profit = df_city_profit.groupBy([\"crime_type\"]).\\\n",
    "                agg(F.sum(\"profit\").alias(\"total_profit\")).\\\n",
    "                orderBy(\"total_profit\", ascending=False)\n",
    "\n",
    "df_by_profit.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_type_big_sales = df_by_profit.select(\"crime_type\").collect()[0][0]\n",
    "crime_type_big_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"crime_type = '{}'\".format(crime_type_big_sales))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city_profit.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_crime_type_profit_df = df_city_profit.where(\"crime_type == '{}'\".format(crime_type_big_sales))\\\n",
    "                    .groupBy([\"country\"])\\\n",
    "                    .agg(F.sum(\"profit\").alias('total_profit'))\\\n",
    "                    .orderBy(\"total_profit\", ascending=False)\n",
    "    \n",
    "countries_crime_type_profit_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_country = countries_crime_type_profit_df.select(\"country\").collect()[0][0]\n",
    "top_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city_profit.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top 5 salesmen in the selected country\n",
    "df_large_sales_alias_null = df_city_profit.where(\"country = '{}' and alias = '' and crime_type = '{}'\".format(top_country, crime_type_big_sales))\\\n",
    "                                            .orderBy(\"profit\", ascending = False)\n",
    "\n",
    "df_large_sales_alias_null.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 3\n",
    "\n",
    "Add date (last deal date) Moriarty does not deal on Sundays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_dates = spark.read.csv(\"./data/id_dates.csv\", header=True, inferSchema=True)\n",
    "print(\"id_dates shape: {}\".format(id_dates.count()))\n",
    "id_dates.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected_with_dates = df_city_profit.join(id_dates, on=[\"id\", \"country\"], how=\"left\")\n",
    "print(df_selected_with_dates.count())\n",
    "df_selected_with_dates.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected_with_dates.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DateType, StringType\n",
    "\n",
    "\n",
    "def weekday(date):\n",
    "    \"\"\" Generate day of the week based on date (as string or as datetime object)\"\"\"\n",
    "    \n",
    "    if isinstance(date, str):\n",
    "        from datetime import datetime\n",
    "        \n",
    "        date = datetime.strptime(date, \"%Y-%m-%d\")  # change the format if necessary\n",
    "        \n",
    "    return date.strftime(\"%A\")\n",
    "\n",
    "\n",
    "weekday_udf = udf(weekday, StringType())\n",
    "\n",
    "# conversion to DateType is not necessary as it is handled inside the function\n",
    "# here it is offered as an example of re-casting\n",
    "df_selected_with_dates = df_selected_with_dates.withColumn(\"date\", F.col(\"date\").cast(DateType()))\n",
    "\n",
    "df_selected_with_dates = df_selected_with_dates.withColumn(\"weekday\", weekday_udf(\"date\").alias(\"weekday\"))\n",
    "df_selected_with_dates.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_type_big_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top 5 salesmen in the selected country\n",
    "df_final = df_selected_with_dates.where(\"\"\"country = '{}' \n",
    "                                            and alias = '' \n",
    "                                            and crime_type = '{}'\n",
    "                                            and weekday != 'Sunday'\n",
    "                                       \"\"\".format(top_country, crime_type_big_sales))\n",
    "\n",
    "df_final.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moriarty_name =  df_final.orderBy(\"profit\", ascending=False).select(\"name\").collect()[0][0]\n",
    "print(\"The name Moriarty is hiding behind: {}\".format(moriarty_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
