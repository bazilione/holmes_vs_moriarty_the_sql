{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first part of the notebook creates tables.\n",
    "# The second part used pysparkSQL to solve the crime.\n",
    "\n",
    "# Just like the 'pyspark_solution' notebook it uses pyspark as the data retrieving framework \n",
    "# but uses complete Hive statements to deal with the data.\n",
    "\n",
    "# The statements, taken separately, could be used to retrieve the data from permanent tables using Hive.\n",
    "\n",
    "# To be able to read and write Hive statements which borrow significant similarity with SQL statements \n",
    "# is a useful skill in the area of data science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SparkSession to use sparksql\n",
    "\n",
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# import data types to cast data and define schema\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, \\\n",
    "                                DecimalType, FloatType\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"hdfs://namenode/sql/metadata/hive\")\\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()\n",
    "\n",
    "hive_folder = \"/Users/vk/Documents/Python/holmes_moriarty_sql/src/hive_data\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# CREATE the main 'criminals' table with the criminals from each country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%rm -rf metastore_db/  # removes any tables that might be presetn from previous runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select database to use and create tables in\n",
    "# 'default' is a pre-created database where we can create tables\n",
    "spark.sql(\"use default\").show(10, False)  \n",
    "# (we could have skipped this statement but it makes it more explicit which database we use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|default  |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initially there are no tables \n",
    "spark.sql(\"show tables\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crime_type_profit_France.txt          criminals_Germany.csv\r\n",
      "crime_type_profit_Germany.txt         criminals_Netherlands.csv\r\n",
      "crime_type_profit_Netherlands.txt     criminals_United Kingdom.csv\r\n",
      "crime_type_profit_United Kingdom.txt  id_dates.csv\r\n",
      "criminals_France.csv\r\n"
     ]
    }
   ],
   "source": [
    "%ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a template that till be used to register the 'criminals' table for each country\n",
    "criminals_country_template=\"\"\"CREATE EXTERNAL TABLE IF NOT EXISTS criminals_{} (\n",
    " id int,\n",
    " name string,\n",
    " alias string,\n",
    " latitude float,\n",
    " longitude float,\n",
    " country string\n",
    "    )\n",
    "STORED AS ORC\n",
    "LOCATION '{}'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_schema(df, schema):\n",
    "    \"\"\"Define data types for data frame columns\"\"\"\n",
    "    \n",
    "    df = spark.createDataFrame(df.rdd, schema=schema)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def rename_cols(df, new_col_names):\n",
    "    \"\"\"\"\"\"\n",
    "    for col, new_col in zip(df.columns, new_col_names):\n",
    "        df = df.withColumnRenamed(col, new_col)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country: United Kingdom, rows: 306\n",
      "Country: Germany, rows: 264\n",
      "Country: Netherlands, rows: 250\n",
      "Country: France, rows: 349\n",
      "+--------+------------------------+-----------+\n",
      "|database|tableName               |isTemporary|\n",
      "+--------+------------------------+-----------+\n",
      "|default |criminals_france        |false      |\n",
      "|default |criminals_germany       |false      |\n",
      "|default |criminals_netherlands   |false      |\n",
      "|default |criminals_united_kingdom|false      |\n",
      "+--------+------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the data\n",
    "\n",
    "#explore the dataframes: column names, shapes and combine into a single dataframe\n",
    "country_list = [\"United Kingdom\", \"Germany\", \"Netherlands\", \"France\"]\n",
    "\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"alias\", StringType(), True),\n",
    "    StructField(\"latitude\", FloatType(), True),\n",
    "    StructField(\"longitude\", FloatType(), True),\n",
    "    StructField(\"country\", StringType(), True)\n",
    "])\n",
    "\n",
    "for country_ in country_list:\n",
    "    file_name = \"./data/criminals_{}.csv\".format(country_)\n",
    "    df = spark.read.csv(file_name, header=True, inferSchema=True)\n",
    "    print(\"Country: {}, rows: {}\".format(country_, df.count()))\n",
    "    new_col_names = [\"id\", \"name\", \"alias\", \"latitude\", \"longitude\"]\n",
    "    df = rename_cols(df, new_col_names)\n",
    "    df = df.withColumn('country', F.lit(country_))\n",
    "    country_ = \"_\".join(country_.split()).lower()  # 'United Kingdom' as space in it and thus is an illigal table name\n",
    "    df = apply_schema(df, schema)\n",
    "    location = os.path.join(hive_folder, 'criminals_{}'.format(country_))\n",
    "    df.persist()\n",
    "    df.write.orc(location, mode=\"overwrite\")\n",
    "    df.unpersist()\n",
    "    \n",
    "    # register external table\n",
    "    spark.sql(criminals_country_template.format(country_, location))\n",
    "    \n",
    "spark.sql(\"show tables\").show(20, False) # isTem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------+-----+--------+---------+--------------+\n",
      "|id |name                    |alias|latitude|longitude|country       |\n",
      "+---+------------------------+-----+--------+---------+--------------+\n",
      "|0  |Ms. Diane Barnett       |null |51.3327 |-0.0328  |United Kingdom|\n",
      "|1  |Elizabeth McDonald      |null |51.3732 |-0.0396  |United Kingdom|\n",
      "|2  |Jacqueline Martin-Winter|null |51.3536 |-0.223   |United Kingdom|\n",
      "+---+------------------------+-----+--------+---------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the table\n",
    "spark.sql(\"select * from criminals_united_kingdom\").show(3, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create crime and profit tables for each country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country: United Kingdom, rows: 306\n",
      "country_:  united_kingdom\n",
      "location:  /Users/vk/Documents/Python/holmes_moriarty_sql/src/hive_data/crime_type_profit_united_kingdom\n",
      "Country: Germany, rows: 264\n",
      "country_:  germany\n",
      "location:  /Users/vk/Documents/Python/holmes_moriarty_sql/src/hive_data/crime_type_profit_germany\n",
      "Country: Netherlands, rows: 250\n",
      "country_:  netherlands\n",
      "location:  /Users/vk/Documents/Python/holmes_moriarty_sql/src/hive_data/crime_type_profit_netherlands\n",
      "Country: France, rows: 349\n",
      "country_:  france\n",
      "location:  /Users/vk/Documents/Python/holmes_moriarty_sql/src/hive_data/crime_type_profit_france\n",
      "+--------+--------------------------------+-----------+\n",
      "|database|tableName                       |isTemporary|\n",
      "+--------+--------------------------------+-----------+\n",
      "|default |crime_type_profit_france        |false      |\n",
      "|default |crime_type_profit_germany       |false      |\n",
      "|default |crime_type_profit_netherlands   |false      |\n",
      "|default |crime_type_profit_united_kingdom|false      |\n",
      "|default |criminals_france                |false      |\n",
      "|default |criminals_germany               |false      |\n",
      "|default |criminals_netherlands           |false      |\n",
      "|default |criminals_united_kingdom        |false      |\n",
      "+--------+--------------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# just as \n",
    "\n",
    "create_table_template=\"\"\"CREATE EXTERNAL TABLE IF NOT EXISTS crime_type_profit_{} (\n",
    " name string,\n",
    " crime_type string,\n",
    " profit string,\n",
    " country string\n",
    "    )\n",
    "STORED AS ORC\n",
    "LOCATION '{}'\n",
    "\"\"\"\n",
    "\n",
    "# get the data\n",
    "\n",
    "#explore the dataframes: column names, shapes and combine into a single dataframe\n",
    "country_list = [\"United Kingdom\", \"Germany\", \"Netherlands\", \"France\"]\n",
    "\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"crime_type\", StringType(), True),\n",
    "    StructField(\"profit\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True)\n",
    "])\n",
    "\n",
    "for country_ in country_list:\n",
    "    file_name = \"./data/crime_type_profit_{}.txt\".format(country_)\n",
    "    df = spark.read.csv(file_name, header=True, inferSchema=True, sep=\" \")\n",
    "    print(\"Country: {}, rows: {}\".format(country_, df.count()))\n",
    "\n",
    "    df = df.withColumn('country', F.lit(country_))\n",
    "    country_ = \"_\".join(country_.split()).lower()  # 'United Kingdom' has space in it and thus is an illigal table name\n",
    "    print(\"country_: \", country_)\n",
    "    location = os.path.join(hive_folder, 'crime_type_profit_{}'.format(country_))\n",
    "    print(\"location: \", location)\n",
    "    df = apply_schema(df, schema)\n",
    "    df.persist()\n",
    "    df.write.orc(location, mode=\"overwrite\")\n",
    "    df.unpersist()\n",
    "    \n",
    "    # register table\n",
    "    spark.sql(create_table_template.format(country_, location))\n",
    "    \n",
    "spark.sql(\"show tables\").show(20, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the table with crime dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id_dates shape: 1169\n",
      "+-----------------------+\n",
      "|count(DISTINCT country)|\n",
      "+-----------------------+\n",
      "|4                      |\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "id_dates = spark.read.csv(\"./data/id_dates.csv\", header=True, inferSchema=True)\n",
    "print(\"id_dates shape: {}\".format(id_dates.count()))\n",
    "id_dates.registerTempTable(\"id_dates\")\n",
    "spark.sql(\"select count(distinct country) from id_dates\").show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------------------+-----------+\n",
      "|database|tableName                       |isTemporary|\n",
      "+--------+--------------------------------+-----------+\n",
      "|default |crime_type_profit_france        |false      |\n",
      "|default |crime_type_profit_germany       |false      |\n",
      "|default |crime_type_profit_netherlands   |false      |\n",
      "|default |crime_type_profit_united_kingdom|false      |\n",
      "|default |criminals_france                |false      |\n",
      "|default |criminals_germany               |false      |\n",
      "|default |criminals_netherlands           |false      |\n",
      "|default |criminals_united_kingdom        |false      |\n",
      "|        |id_dates                        |true       |\n",
      "+--------+--------------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check available tables ones more\n",
    "spark.sql(\"show tables\").show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE GOAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is using the intel (data in the supplied files) from the police, Interpol, and undercover agents about Europe's criminals to identify the name behind which Moriarty is hiding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOLUTION. PART 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 1\n",
    "-Watson, just like our grand-grand-fathers we are again after Moriarty.\n",
    "\n",
    "We need to catch him... maybe it is her. All we know is that someone is masterminding unlawful activities and planning something bad. The Interpol agents, with the help of my boys, collected information that should provide us the clues to determine the name Moriarty is hiding behind, and arrest him.\n",
    "\n",
    "-The data is in the csv and text files and contains info on the criminal activity in the last year as well as high-profile and suspicious sales. They were sent over by our collegues from the neighboring countries: France, Germany, Netherlands, and our own MI-6 in the United Kingdom.\n",
    "\n",
    "-The first task would be to combine the data into one table. I requested the information on the name, alias, and the location of the last known whereabouts of the criminals, as latitude and longitude, but since the data comes from all around the Europe the columns names may differ between files.\n",
    "\n",
    "-I am thinking that adding the country to the data might be helpful in our future analysis.\n",
    "\n",
    "-Lastly, from my correspondence with our undercover agents, all the activity seems to be happening around major financial centers. If those are not in the data, I suppose you can extract the city names using the latitude and logitude. And a map of course, unless your knowledge of Europe's geography is excepitonal.\n",
    "\n",
    "Data tasks outline:\n",
    "1. Read the data from the files (named 'criminals_' plus country name) into separate dataframes and add the country name as 'country' column.\n",
    "2. Identify the city around which the criminals operate and add it to the dataframe as 'city' column.\n",
    "3. Concatenate the dataframes into a single dataframe with the four original columns renamed to: [name, alias, latitude, longitude]\n",
    "4. Fill NAs in aliases with an empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "criminals_unioned count: 1169\n",
      "+---+--------------------+-----+--------+---------+-------+\n",
      "| id|                name|alias|latitude|longitude|country|\n",
      "+---+--------------------+-----+--------+---------+-------+\n",
      "|  0|Henriette Thomas ...| null| 48.9072|   2.2521| France|\n",
      "|190|Inès Martins de l...| null| 49.0029|    2.251| France|\n",
      "|215|         Isaac Labbe| null| 48.8873|   2.3601| France|\n",
      "+---+--------------------+-----+--------+---------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# union the data for all countries and register the result as separate temporary table\n",
    "df_criminals_combined = spark.sql(\"\"\"select * from criminals_france\n",
    "          union\n",
    "          select * from criminals_germany\n",
    "          union\n",
    "          select * from criminals_netherlands\n",
    "          union\n",
    "          select * from criminals_united_kingdom\"\"\")\n",
    "\n",
    "print(\"criminals_unioned count: {}\".format(df_criminals_combined.count()))\n",
    "\n",
    "df_criminals_combined.registerTempTable(\"criminals\")\n",
    "\n",
    "df_criminals_combined.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+--------------------+\n",
      "|country       |avg_lat           |avg_lon             |\n",
      "+--------------+------------------+--------------------+\n",
      "|France        |48.86060943166301 |2.364566761989648   |\n",
      "|Germany       |50.097135254831024|8.678964380061988   |\n",
      "|Netherlands   |52.37530560302734 |4.900951610565185   |\n",
      "|United Kingdom|51.50456336276983 |-0.12400588218790493|\n",
      "+--------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculate mean latitude and longitude to identify the major financial centers (cities)\n",
    "# (copy and paste the lat, lon values into Google Maps)\n",
    "# dataframe.filter(df['salary'] > 100000).agg({\"age\": \"avg\"})\n",
    "\n",
    "spark.sql(\"\"\"select country, \n",
    "                    AVG(latitude) as avg_lat, \n",
    "                    AVG(longitude) as avg_lon\n",
    "                    from criminals\n",
    "                    group by country\n",
    "                    order by country\"\"\").show(10, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+-------+\n",
      "|country       |avg_lat|avg_lon|\n",
      "+--------------+-------+-------+\n",
      "|France        |48.8606|2.3646 |\n",
      "|Germany       |50.0971|8.679  |\n",
      "|Netherlands   |52.3753|4.901  |\n",
      "|United Kingdom|51.5046|-0.124 |\n",
      "+--------------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select country,\n",
    "                    ROUND(AVG(latitude), 4) as avg_lat,\n",
    "                    ROUND(AVG(longitude), 4) as avg_lon\n",
    "                from criminals\n",
    "                 group by country\n",
    "                 order by country\"\"\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------------+-----+--------+---------+-----------+---------+\n",
      "|id |name                     |alias|latitude|longitude|country    |city     |\n",
      "+---+-------------------------+-----+--------+---------+-----------+---------+\n",
      "|0  |Henriette Thomas du Peron|null |48.9072 |2.2521   |France     |Paris    |\n",
      "|190|Inès Martins de la Morel |null |49.0029 |2.251    |France     |Paris    |\n",
      "|215|Isaac Labbe              |null |48.8873 |2.3601   |France     |Paris    |\n",
      "|244|Aimée-Margot Martins     |null |48.885  |2.5288   |France     |Paris    |\n",
      "|297|Olivier Lopez            |Flako|48.616  |2.1133   |France     |Paris    |\n",
      "|326|Philippine Traore        |null |49.0171 |2.4338   |France     |Paris    |\n",
      "|23 |Dipl.-Ing. Dennis Hesse  |null |50.0266 |8.6771   |Germany    |Frankfurt|\n",
      "|202|Ali Dowerg               |null |50.1855 |8.5548   |Germany    |Frankfurt|\n",
      "|243|Tomas Mies               |null |50.0033 |8.9106   |Germany    |Frankfurt|\n",
      "|232|Liam Janssen-Frankhuizen |null |52.5289 |4.8684   |Netherlands|Amsterdam|\n",
      "+---+-------------------------+-----+--------+---------+-----------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# add the city name to the df\n",
    "spark.sql(\"\"\"select *, \n",
    "                case \n",
    "                    when country = 'United Kingdom' then 'London'\n",
    "                    when country = 'France' then 'Paris'\n",
    "                    when country = 'Germany' then 'Frankfurt'\n",
    "                    when country = 'Netherlands' then 'Amsterdam'\n",
    "                end as city\n",
    "            from criminals\"\"\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------------+-----+-----------+---------+\n",
      "|id |name                     |alias|country    |city     |\n",
      "+---+-------------------------+-----+-----------+---------+\n",
      "|0  |Henriette Thomas du Peron|     |France     |Paris    |\n",
      "|190|Inès Martins de la Morel |     |France     |Paris    |\n",
      "|215|Isaac Labbe              |     |France     |Paris    |\n",
      "|244|Aimée-Margot Martins     |     |France     |Paris    |\n",
      "|297|Olivier Lopez            |Flako|France     |Paris    |\n",
      "|326|Philippine Traore        |     |France     |Paris    |\n",
      "|23 |Dipl.-Ing. Dennis Hesse  |     |Germany    |Frankfurt|\n",
      "|202|Ali Dowerg               |     |Germany    |Frankfurt|\n",
      "|243|Tomas Mies               |     |Germany    |Frankfurt|\n",
      "|232|Liam Janssen-Frankhuizen |     |Netherlands|Amsterdam|\n",
      "+---+-------------------------+-----+-----------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fill nulls with an empty string \n",
    "# we'll also assign this new data to a variable name for saving and creating a new table to use later\n",
    "# (note that 'show' method is moved to the spark dataframe)\n",
    "criminals_with_city_df = spark.sql(\"\"\"select id, name,\n",
    "                case\n",
    "                    when alias is null then ''\n",
    "                    else alias\n",
    "                end as alias,\n",
    "                country,\n",
    "                case \n",
    "                    when country = 'United Kingdom' then 'London'\n",
    "                    when country = 'France' then 'Paris'\n",
    "                    when country = 'Germany' then 'Frankfurt'\n",
    "                    when country = 'Netherlands' then 'Amsterdam'\n",
    "                end as city\n",
    "            from criminals\"\"\")\n",
    "\n",
    "criminals_with_city_df.cache()\n",
    "criminals_with_city_df.registerTempTable(\"criminals_with_city\")\n",
    "\n",
    "criminals_with_city_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(country='Germany'),\n",
       " Row(country='France'),\n",
       " Row(country='United Kingdom'),\n",
       " Row(country='Netherlands')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that the data is readable\n",
    "spark.sql(\"select distinct country from criminals_with_city\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "Add crime_type and profit info to criminals. \n",
    "#(merge/join) criminals table with the crime type and profit information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Great, Watson! \n",
    "- Now we need to know what everyone of those supspects did wrong, that is the crime type, and desirably, how much they profited from it: Moriarty is not a small fish. He is in the category with th largest total sales.\n",
    "\n",
    "- You'll need to add the crime type and the profit from the files to the table you already put together. Be mindful of the file types. I also believe that the separator in these file maybe different from the files you used previously.\n",
    "-Moriarty made one of the top 5 sales last year. He is not stupid for nicknames, I am pretty sure he doesn't have an alias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution (task 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------------------+-----------+\n",
      "|database|tableName                       |isTemporary|\n",
      "+--------+--------------------------------+-----------+\n",
      "|default |crime_type_profit_france        |false      |\n",
      "|default |crime_type_profit_germany       |false      |\n",
      "|default |crime_type_profit_netherlands   |false      |\n",
      "|default |crime_type_profit_united_kingdom|false      |\n",
      "|default |criminals_france                |false      |\n",
      "|default |criminals_germany               |false      |\n",
      "|default |criminals_netherlands           |false      |\n",
      "|default |criminals_united_kingdom        |false      |\n",
      "|        |criminals                       |true       |\n",
      "|        |criminals_with_city             |true       |\n",
      "|        |id_dates                        |true       |\n",
      "+--------+--------------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the tables that currently available\n",
    "spark.sql(\"show tables\").show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crime_profit count: 1169\n",
      "+---------------+-------------+------+-----------+\n",
      "|           name|   crime_type|profit|    country|\n",
      "+---------------+-------------+------+-----------+\n",
      "|    Isaac David|        theft|   204|     France|\n",
      "|Nikolaj Kallert|pickpocketing|    47|    Germany|\n",
      "|   Leah Winters|    drug sale|  3510|Netherlands|\n",
      "+---------------+-------------+------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crime_profit = spark.sql(\"\"\"select * from crime_type_profit_france\n",
    "          union\n",
    "          select * from crime_type_profit_germany\n",
    "          union\n",
    "          select * from crime_type_profit_netherlands\n",
    "          union\n",
    "          select * from crime_type_profit_united_kingdom\"\"\")\n",
    "\n",
    "print(\"crime_profit count: {}\".format(crime_profit.count()))\n",
    "\n",
    "crime_profit.registerTempTable(\"crime_profit\")\n",
    "\n",
    "crime_profit.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|country_count|\n",
      "+-------------+\n",
      "|4            |\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# table check\n",
    "spark.sql(\"select count(distinct country) as country_count from crime_profit\").show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name', 'crime_type', 'profit', 'country']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check columns\n",
    "spark.sql(\"select * from crime_profit\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------------+-----+-------------+------+-----------+---------+\n",
      "|id |name                     |alias|crime_type   |profit|country    |city     |\n",
      "+---+-------------------------+-----+-------------+------+-----------+---------+\n",
      "|0  |Henriette Thomas du Peron|     |robbery      |558   |France     |Paris    |\n",
      "|190|Inès Martins de la Morel |     |pickpocketing|44    |France     |Paris    |\n",
      "|215|Isaac Labbe              |     |drug sale    |37260 |France     |Paris    |\n",
      "|244|Aimée-Margot Martins     |     |pickpocketing|26    |France     |Paris    |\n",
      "|297|Olivier Lopez            |Flako|theft        |318   |France     |Paris    |\n",
      "|326|Philippine Traore        |     |theft        |320   |France     |Paris    |\n",
      "|23 |Dipl.-Ing. Dennis Hesse  |     |theft        |44    |Germany    |Frankfurt|\n",
      "|202|Ali Dowerg               |     |theft        |211   |Germany    |Frankfurt|\n",
      "|243|Tomas Mies               |     |pickpocketing|27    |Germany    |Frankfurt|\n",
      "|232|Liam Janssen-Frankhuizen |     |theft        |48    |Netherlands|Amsterdam|\n",
      "+---+-------------------------+-----+-------------+------+-----------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select  a.id, a.name, a.alias, b.crime_type, b.profit, b.country, a.city\n",
    "            from criminals_with_city a\n",
    "            left join crime_profit b\n",
    "                on a.name = b.name and a.country = b.country\"\"\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|row_count|\n",
      "+---------+\n",
      "|1169     |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select count(*) as row_count from (\n",
    "                select  a.id, a.name, a.alias, b.crime_type, b.profit, b.country\n",
    "                from criminals a\n",
    "                left join crime_profit b\n",
    "                    on a.name = b.name and a.country = b.country)\"\"\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------+----------+------------+------+--------------+\n",
      "|id |name                    |alias     |crime_type  |profit|country       |\n",
      "+---+------------------------+----------+------------+------+--------------+\n",
      "|302|Odette Renard du Michaud|null      |weapons sale|498000|France        |\n",
      "|58 |Anthony Mitchell        |null      |weapons sale|495000|United Kingdom|\n",
      "|307|Gabriel Le Schneider    |null      |weapons sale|493000|France        |\n",
      "|62 |Malcolm Cox-Mason       |Handlebars|weapons sale|491000|United Kingdom|\n",
      "|25 |Lily Walter             |Montana   |weapons sale|484000|Netherlands   |\n",
      "|174|Univ.Prof. Augusta Putz |Kiki      |weapons sale|474000|Germany       |\n",
      "|203|Gordon Parker           |null      |weapons sale|467000|United Kingdom|\n",
      "|245|Andrew Smith            |null      |weapons sale|466000|United Kingdom|\n",
      "|171|Constance du Laurent    |null      |weapons sale|453000|France        |\n",
      "|200|Valentine Meunier       |null      |weapons sale|435000|France        |\n",
      "+---+------------------------+----------+------------+------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# order by profit (descending) and \n",
    "# cast profit as int (to keep the same with pyspark notebook; not necessary here)\n",
    "spark.sql(\"\"\"select  a.id, a.name, a.alias, b.crime_type, CAST(b.profit AS INT), b.country\n",
    "            from criminals a\n",
    "            left join crime_profit b\n",
    "                on a.name = b.name and a.country = b.country\n",
    "            order by profit DESC\"\"\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+\n",
      "|crime_type   |total_profit|\n",
      "+-------------+------------+\n",
      "|weapons sale |14942000    |\n",
      "|drug sale    |2214270     |\n",
      "|robbery      |96582       |\n",
      "|theft        |95702       |\n",
      "|forgery      |37863       |\n",
      "|pickpocketing|6359        |\n",
      "+-------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find the most profitable crime type\n",
    "\n",
    "spark.sql(\"\"\"select crime_type, CAST(sum(profit) AS INT) as total_profit\n",
    "            from criminals a\n",
    "            left join crime_profit b\n",
    "                on a.name = b.name and a.country = b.country\n",
    "            group by crime_type\n",
    "            order by total_profit DESC\"\"\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------------------+-----------+\n",
      "|database|tableName                       |isTemporary|\n",
      "+--------+--------------------------------+-----------+\n",
      "|default |crime_type_profit_france        |false      |\n",
      "|default |crime_type_profit_germany       |false      |\n",
      "|default |crime_type_profit_netherlands   |false      |\n",
      "|default |crime_type_profit_united_kingdom|false      |\n",
      "|default |criminals_france                |false      |\n",
      "|default |criminals_germany               |false      |\n",
      "|default |criminals_netherlands           |false      |\n",
      "|default |criminals_united_kingdom        |false      |\n",
      "|        |crime_profit                    |true       |\n",
      "|        |criminals                       |true       |\n",
      "|        |criminals_with_city             |true       |\n",
      "|        |id_dates                        |true       |\n",
      "+--------+--------------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# remind ourselves the tables\n",
    "spark.sql(\"show tables\").show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------+\n",
      "|country       |total_profit|\n",
      "+--------------+------------+\n",
      "|France        |6312000     |\n",
      "|United Kingdom|3914000     |\n",
      "|Germany       |2365000     |\n",
      "|Netherlands   |2351000     |\n",
      "+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find the country where with the highest weapons sales\n",
    "\n",
    "spark.sql(\"\"\"select a.country, CAST(sum(profit) AS INT) as total_profit\n",
    "            from criminals a\n",
    "            left join crime_profit b\n",
    "                on a.name = b.name and a.country = b.country\n",
    "            where crime_type = 'weapons sale'\n",
    "            group by a.country\n",
    "            order by total_profit DESC\n",
    "            \"\"\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add crime and profit and register new table\n",
    "\n",
    "criminals_with_profit = spark.sql(\"\"\"select  a.id, a.name, a.alias, b.crime_type, b.profit, b.country\n",
    "            from criminals_with_city a\n",
    "            left join crime_profit b\n",
    "                on a.name = b.name and a.country = b.country\"\"\")\n",
    "\n",
    "criminals_with_profit.cache()\n",
    "criminals_with_profit.registerTempTable(\"criminals_with_profit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------------------+-----------+\n",
      "|database|tableName                       |isTemporary|\n",
      "+--------+--------------------------------+-----------+\n",
      "|default |crime_type_profit_france        |false      |\n",
      "|default |crime_type_profit_germany       |false      |\n",
      "|default |crime_type_profit_netherlands   |false      |\n",
      "|default |crime_type_profit_united_kingdom|false      |\n",
      "|default |criminals_france                |false      |\n",
      "|default |criminals_germany               |false      |\n",
      "|default |criminals_netherlands           |false      |\n",
      "|default |criminals_united_kingdom        |false      |\n",
      "|        |crime_profit                    |true       |\n",
      "|        |criminals                       |true       |\n",
      "|        |criminals_with_city             |true       |\n",
      "|        |criminals_with_profit           |true       |\n",
      "|        |id_dates                        |true       |\n",
      "+--------+--------------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------------+-----+-------------+------+-------+\n",
      "|id |name                     |alias|crime_type   |profit|country|\n",
      "+---+-------------------------+-----+-------------+------+-------+\n",
      "|0  |Henriette Thomas du Peron|     |robbery      |558   |France |\n",
      "|190|Inès Martins de la Morel |     |pickpocketing|44    |France |\n",
      "|215|Isaac Labbe              |     |drug sale    |37260 |France |\n",
      "|244|Aimée-Margot Martins     |     |pickpocketing|26    |France |\n",
      "|297|Olivier Lopez            |Flako|theft        |318   |France |\n",
      "+---+-------------------------+-----+-------------+------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from criminals_with_profit\").show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------+------+\n",
      "|id |name                    |profit|\n",
      "+---+------------------------+------+\n",
      "|302|Odette Renard du Michaud|498000|\n",
      "|307|Gabriel Le Schneider    |493000|\n",
      "|171|Constance du Laurent    |453000|\n",
      "|200|Valentine Meunier       |435000|\n",
      "|19 |René Tessier du Lagarde |423000|\n",
      "+---+------------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select top 5 to view top criminals in the selected country with selected sales\n",
    "\n",
    "spark.sql(\"\"\"select id, name, CAST(profit as INT) \n",
    "            from criminals_with_profit \n",
    "            where crime_type LIKE 'weapon%' \n",
    "                and country = 'France'\n",
    "            order by profit DESC\n",
    "            \"\"\").show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 3\n",
    "\n",
    "-Watson, I think we got the last piece of the puzzle!\n",
    "\n",
    "I learned that Moriarty doesn't do his dealings on Sunday.\n",
    "\n",
    "That means that the top seller (in the country with the top sale in the last year) who didn't sell on a Sunday and who doesn't have an aliase will be him.\n",
    "\n",
    "All we have to do now is add the date information I just got and determine the weekday for that date. We already know the rest.\n",
    "\n",
    "And we'll send Lestrade right after him!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------+-------+\n",
      "|id |date      |country|day_num|\n",
      "+---+----------+-------+-------+\n",
      "|0  |2020-06-15|France |0      |\n",
      "|1  |2020-01-06|France |0      |\n",
      "|2  |2020-08-03|France |0      |\n",
      "|3  |2020-06-19|France |4      |\n",
      "|4  |2020-11-20|France |4      |\n",
      "+---+----------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select *, weekday(date) as day_num from id_dates\").show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------+-------+\n",
      "|id |date      |country|day_num|\n",
      "+---+----------+-------+-------+\n",
      "|0  |2020-06-15|France |0      |\n",
      "|1  |2020-01-06|France |0      |\n",
      "|2  |2020-08-03|France |0      |\n",
      "|3  |2020-06-19|France |4      |\n",
      "|4  |2020-11-20|France |4      |\n",
      "+---+----------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select *, weekday(date) as day_num from id_dates\").registerTempTable(\"dates_with_day_number\")\n",
    "spark.sql(\"select * from dates_with_day_number\").show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|num|\n",
      "+---+\n",
      "|0  |\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use a known date to determine how day numbering is setup by default\n",
    "# 2020-12-14 is Monday\n",
    "spark.sql(\"select weekday('2020-12-14') as num\").show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------+-------+--------+\n",
      "|id |date      |country|day_num|weekday |\n",
      "+---+----------+-------+-------+--------+\n",
      "|0  |2020-06-15|France |0      |Monday  |\n",
      "|1  |2020-01-06|France |0      |Monday  |\n",
      "|2  |2020-08-03|France |0      |Monday  |\n",
      "|3  |2020-06-19|France |4      |Friday  |\n",
      "|4  |2020-11-20|France |4      |Friday  |\n",
      "|5  |2020-01-11|France |5      |Saturday|\n",
      "|6  |2020-04-23|France |3      |Thursday|\n",
      "|7  |2020-09-27|France |6      |Sunday  |\n",
      "|8  |2020-05-08|France |4      |Friday  |\n",
      "|9  |2020-04-09|France |3      |Thursday|\n",
      "+---+----------+-------+-------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select *, \n",
    "            case day_num\n",
    "             when 0  then 'Monday'\n",
    "             when 1 then'Tuesday'\n",
    "             when 2 then 'Wednesday'\n",
    "             when 3 then 'Thursday'\n",
    "             when 4 then'Friday'\n",
    "             when 5 then 'Saturday'\n",
    "             when 6 then 'Sunday'\n",
    "        end as weekday\n",
    "        from dates_with_day_number\"\"\").show(10, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------+-------+\n",
      "|id |date      |weekday |country|\n",
      "+---+----------+--------+-------+\n",
      "|0  |2020-06-15|Monday  |France |\n",
      "|1  |2020-01-06|Monday  |France |\n",
      "|2  |2020-08-03|Monday  |France |\n",
      "|3  |2020-06-19|Friday  |France |\n",
      "|4  |2020-11-20|Friday  |France |\n",
      "|5  |2020-01-11|Saturday|France |\n",
      "|6  |2020-04-23|Thursday|France |\n",
      "|7  |2020-09-27|Sunday  |France |\n",
      "|8  |2020-05-08|Friday  |France |\n",
      "|9  |2020-04-09|Thursday|France |\n",
      "+---+----------+--------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select only needed columns and register a table\n",
    "df = spark.sql(\"\"\"select id, date,\n",
    "            case day_num\n",
    "             when 0  then 'Monday'\n",
    "             when 1 then'Tuesday'\n",
    "             when 2 then 'Wednesday'\n",
    "             when 3 then 'Thursday'\n",
    "             when 4 then'Friday'\n",
    "             when 5 then 'Saturday'\n",
    "             when 6 then 'Sunday'\n",
    "        end as weekday,\n",
    "        country\n",
    "        from dates_with_day_number\"\"\")\n",
    "\n",
    "df.registerTempTable(\"id_dates_with_weekday\")\n",
    "\n",
    "df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------------+-----+-------------+------+-------+--------+\n",
      "|id |name                     |alias|crime_type   |profit|country|weekday |\n",
      "+---+-------------------------+-----+-------------+------+-------+--------+\n",
      "|0  |Henriette Thomas du Peron|     |robbery      |558   |France |Monday  |\n",
      "|190|Inès Martins de la Morel |     |pickpocketing|44    |France |Friday  |\n",
      "|215|Isaac Labbe              |     |drug sale    |37260 |France |Thursday|\n",
      "|244|Aimée-Margot Martins     |     |pickpocketing|26    |France |Friday  |\n",
      "|297|Olivier Lopez            |Flako|theft        |318   |France |Sunday  |\n",
      "+---+-------------------------+-----+-------------+------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"\"\"select a.*, b.weekday from \n",
    "                criminals_with_profit a\n",
    "                inner join id_dates_with_weekday b \n",
    "                on a.id = b.id \n",
    "                and a.country = b.country\"\"\")\n",
    "\n",
    "df.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.registerTempTable(\"criminals_with_weekday\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+------+\n",
      "|name                    |profit|\n",
      "+------------------------+------+\n",
      "|Odette Renard du Michaud|498000|\n",
      "|Constance du Laurent    |453000|\n",
      "|René Tessier du Lagarde |423000|\n",
      "|Zoé Guibert de la Levy  |364000|\n",
      "|Denis Lesage            |328000|\n",
      "|Henriette Charpentier   |282000|\n",
      "|Thomas Couturier        |267000|\n",
      "|daisy Toussaint         |266000|\n",
      "|Suzanne Levy            |216000|\n",
      "|Zacharie Samson         |208000|\n",
      "+------------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show top 5 salesmen in the selected country\n",
    "df_final = spark.sql(\"\"\"select name, CAST(profit AS INT)\n",
    "                            from criminals_with_weekday\n",
    "                            where\n",
    "                                country = 'France' \n",
    "                                and alias = '' \n",
    "                                and crime_type = 'weapons sale'\n",
    "                                and weekday != 'Sunday'\n",
    "                                order by profit DESC\"\"\")\n",
    "\n",
    "df_final.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The name Moriarty is hiding behind: Odette Renard du Michaud\n"
     ]
    }
   ],
   "source": [
    "\n",
    "moriarty_name =  df_final.select(\"name\").collect()[0][0]\n",
    "print(\"The name Moriarty is hiding behind: {}\".format(moriarty_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# moriarty_name = Odette Renard du Michaud"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
