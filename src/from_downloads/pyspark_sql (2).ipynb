{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE GOAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to take Watson's role and using the intel (the data in the supplied files) from the police, Interpol, and undercover agents about Europe's criminals to identify the name behind which Moriarty is hiding. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1\n",
    "-Watson, just like our grand-grand-fathers we are again after Moriarty. \n",
    "\n",
    "We need to catch him. H-mmm... I need to be careful here - maybe it is not him, maybe it is her. All we know is \n",
    "that someone is masterminding unlawful activities and planning something bad. The Interpol agents, with the help of my boys, collected information that should provide us the clues to determine the name Moriarty's is hiding brhind and arrest him.\n",
    "\n",
    "-I have a number of .csv and .txt files about criminal activity and high-profile suspicious sales that were sent over from our neighbors: France, Germany, Netherlands, and our own MI-6 in the United Kingdom.\n",
    "\n",
    "So, the first task would be to combine the data into one table. I requested info on the name, alias, and the location of the last known whereabouts, as latitude and longitude, but since the data comes from all around the Europe they might have named the columns differently.\n",
    "\n",
    "I am thinking that adding the country to the data might be helpful in our future analysis.\n",
    "\n",
    "Lastly, from my correspondence with our undercover agents, all the activity seems to be happening around major financial centers. If the city names are not in the data, I suppose you can extract it based on the latitude and logitude. Mmmm... And a map of course, unless your knowledge of Europe's geography is excepitonal. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Text:\n",
    "Tasks:\n",
    "1. Read in data from the files into a separate dataframe and add the country name ('country' column).\n",
    "2. Identify the city around which the criminals operate. Add it to the dataframe ('city' column).\n",
    "3. Concatenate dfs into a single dataframe with the four original columns renamed to: [name, alias, latitude, longitude]\n",
    "4. Fill NAs in aliases with an empty string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0.1\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "print(pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# from https://datascience.stackexchange.com/questions/11356/merging-multiple-data-frames-row-wise-in-pyspark\n",
    "from functools import reduce  # For Python 3.x\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"hdfs://namenode/sql/metadata/hive\")\\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()\n",
    "# .config(\"spark.sql.warehouse.dir\", \"hdfs://namenode/sql/metadata/hive\") \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|default  |\n",
      "+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select database to use and create tables in\n",
    "spark.sql(\"show databases\").show(10, False)\n",
    "spark.sql(\"use default\")  # 'default' is a pre-created database where we can create tables\n",
    "# (we could have skipped this statement but it makes it more explicity which database we use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# we could also create our tables in our own custom database using:\n",
    "# spark.sql(\"create database moriarty_db\")\n",
    "# spark.sql(\"use moriarty_db\")\n",
    "spark.sql(\"show tables\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: data: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "%ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Path does not exist: file:/Users/vk/Documents/Python/holmes_moriarty_sql/src/from_downloads/data/criminals_United Kingdom.csv;",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f2d4c49e2086>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcountry_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcountry_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./data/criminals_{}.csv\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcountry_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Country: {}, rows: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcountry_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mnew_col_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"name\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"alias\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"latitude\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"longitude\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Python/holmes_moriarty_sql/env/lib/python3.6/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup)\u001b[0m\n\u001b[1;32m    533\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Python/holmes_moriarty_sql/env/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Python/holmes_moriarty_sql/env/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Python/holmes_moriarty_sql/env/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: file:/Users/vk/Documents/Python/holmes_moriarty_sql/src/from_downloads/data/criminals_United Kingdom.csv;"
     ]
    }
   ],
   "source": [
    "# get the data\n",
    "\n",
    "def rename_cols(df, new_col_names):\n",
    "    \"\"\"\"\"\"\n",
    "    for col, new_col in zip(df.columns, new_col_names):\n",
    "        df = df.withColumnRenamed(col, new_col)\n",
    "        \n",
    "    return df\n",
    "\n",
    "#explore the dataframes: column names, shapes and combine into a single dataframe\n",
    "country_list = [\"United Kingdom\", \"Germany\", \"Netherlands\", \"France\"]\n",
    "dfs_dict = {}\n",
    "for country_ in country_list:\n",
    "    file_name = \"./data/criminals_{}.csv\".format(country_)\n",
    "    df = spark.read.csv(file_name, header=True, inferSchema=True)\n",
    "    print(\"Country: {}, rows: {}\".format(country_, df.count()))\n",
    "    new_col_names = [\"id\", \"name\", \"alias\", \"latitude\", \"longitude\"]\n",
    "    df = rename_cols(df, new_col_names)\n",
    "    df = df.withColumn('country', F.lit(country_))\n",
    "    country_ = \"_\".join(country_.split())  # 'United Kingdom' as space in it and thus is an illigal table name\n",
    "    df.registerTempTable(\"criminals_{}\".format(country_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unionAll(dfs):\n",
    "    return reduce(DataFrame.unionAll, dfs)\n",
    "\n",
    "df_criminals_combined = unionAll(list(dfs_dict.values()))\n",
    "print(\"Rows in combined df: {}\".format(df_criminals_combined.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as ORC file (a popular data format in big data management)\n",
    "df_criminals_combined.cache()\n",
    "df_criminals_combined.coalesce(1).write.orc(\"./sql_data/criminals\", mode='overwrite') # doesn't have 'orc' extension as it is a folder\n",
    "# the file with extension '.orc' will be inside it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check schema before casting and saving\n",
    "df_criminals_combined.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# often to insure that datatypes are compatible for retrieval via Hive we need to explicitly define data types\n",
    "\n",
    "# import data types to cast data and define schema\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, \\\n",
    "                                DecimalType, FloatType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"alias\", StringType(), True),\n",
    "    StructField(\"latitude\", FloatType(), True),\n",
    "    StructField(\"longitude\", FloatType(), True),\n",
    "    StructField(\"country\", StringType(), True)\n",
    "])\n",
    "\n",
    "# apply schema\n",
    "df_criminals_combined_new_schema = spark.createDataFrame(df_criminals_combined.rdd, schema=schema)\n",
    "df_criminals_combined_new_schema.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a table that defines the data (including the data location)\n",
    "\n",
    "# drop table if we created it before and define the table by registering 'table definition'\n",
    "spark.sql(\"drop table if exists criminals\")\n",
    "\n",
    "# the hive datatypes should be appropriate (not necessarily identically named to spark datatypes)\n",
    "\n",
    "# 'EXTERNAL' (also could be 'external') makes sure that if the table is dropped (deleted) the data remains\n",
    "spark.sql(\"\"\"CREATE EXTERNAL TABLE criminals (\n",
    " id int,\n",
    " name string,\n",
    " alias string,\n",
    " latitude float,\n",
    " longitude float,\n",
    " country string\n",
    "    )\n",
    "STORED AS ORC\n",
    "LOCATION '/Users/vk/Documents/Python/holmes_moriarty_sql/src/sql_data/criminals'\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criminals_df = spark.sql(\"select * from criminals\")\n",
    "# print(\"Table (read-in) count: {}\".format(criminals_df.count()))\n",
    "criminals_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criminals_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate mean latitude and longitude to identify the major financial centers (cities)\n",
    "# (copy and paste the lat, lon values into Google Maps)\n",
    "# dataframe.filter(df['salary'] > 100000).agg({\"age\": \"avg\"})\n",
    "\n",
    "spark.sql(\"\"\"select country, \n",
    "                    AVG(latitude) as avg_lat, \n",
    "                    AVG(longitude) as avg_lon\n",
    "                    from criminals\n",
    "                    group by country\n",
    "                    order by country\"\"\").show(10, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"select country,\n",
    "                    ROUND(AVG(latitude), 4) as avg_lat,\n",
    "                    ROUND(AVG(longitude), 4) as avg_lon\n",
    "                from criminals\n",
    "                 group by country\n",
    "                 order by country\"\"\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the city name to the df\n",
    "\n",
    "#it can be done using a series of if/else statements, such as 'if country_ == 'France': city = 'Paris', etc. OR\n",
    "# using a dictionary as below:\n",
    "country_city_dict = {\"United Kingdom\": \"London\", \"Germany\": \"Frankfurt\", \"Netherlands\": \"Amsterdam\", \"France\": \"Paris\"}\n",
    "country_city_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"select *, \n",
    "                case \n",
    "                    when country = 'United Kingdom' then 'London'\n",
    "                    when country = 'France' then 'Paris'\n",
    "                    when country = 'Germany' then 'Frankfurt'\n",
    "                    when country = 'Netherlands' then 'Amsterdam'\n",
    "                end as city\n",
    "            from criminals\"\"\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill Nas with empty string \n",
    "# we'll also assign this new data to a variable name for saving and creating a new table to use later\n",
    "# (note that 'show' method is moved to the spark dataframe)\n",
    "criminals_with_city = spark.sql(\"\"\"select id, name,\n",
    "                case\n",
    "                    when alias is null then ''\n",
    "                    else alias\n",
    "                end as alias,\n",
    "                country,\n",
    "                case \n",
    "                    when country = 'United Kingdom' then 'London'\n",
    "                    when country = 'France' then 'Paris'\n",
    "                    when country = 'Germany' then 'Frankfurt'\n",
    "                    when country = 'Netherlands' then 'Amsterdam'\n",
    "                end as city\n",
    "            from criminals\"\"\")\n",
    "\n",
    "criminals_with_city.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criminals_with_city.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criminals_with_city.cache()\n",
    "criminals_with_city.coalesce(1).write.orc(\"./sql_data/criminals_with_city\", mode='overwrite') # doesn't have 'orc' extension as it is a folder\n",
    "# the file with extension '.orc' will be inside it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"drop table if exists criminals_with_city\")\n",
    "\n",
    "spark.sql(\"\"\"CREATE EXTERNAL TABLE criminals_with_city (\n",
    " id int,\n",
    " name string,\n",
    " alias string,\n",
    " country string,\n",
    " city string)\n",
    "STORED AS ORC\n",
    "LOCATION '/Users/vk/Documents/Python/holmes_moriarty_sql/src/sql_data/criminals_with_city'\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the data is readable\n",
    "spark.sql(\"select * from criminals_with_city\").show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "Add crime_type and profit info to criminals. \n",
    "#(merge/join) criminals table with the crime type and profit information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Great, Watson! \n",
    "- Now we need to know what everyone of those supspects did wrong, that is the crime type, and desirably, how much they profited from it: Moriarty is not a small fish. He is in the category with th largest total sales.\n",
    "\n",
    "- You'll need to add the crime type and the profit from the files to the table you already put together. Be mindful of the file types. I also believe that the separator in these file maybe different from the files you used previously.\n",
    "-Moriarty made one of the top 5 sales last year. He is not stupid for nicknames, I am pretty sure he doesn't have an alias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution (task 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# union(concatenate) files for the latest crime dates\n",
    "\n",
    "country_list = [\"United Kingdom\", \"Germany\", \"Netherlands\", \"France\"]\n",
    "\n",
    "for country_ in country_list:\n",
    "    file_name = \"./data/crime_type_profit_{}.txt\".format(country_)\n",
    "    df = spark.read.csv(file_name, header=True, sep=\" \")\n",
    "    print(\"rows: {}\".format(df.count()))\n",
    "    df = df.withColumn('country', F.lit(country_))\n",
    "    country_ = \"_\".join(country_.split())  # 'United Kingdom' as space in it and thus is an illigal table name\n",
    "    df.registerTempTable(\"crime_profit_{}\".format(country_))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"use default\")\n",
    "spark.sql(\"show tables\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from criminals_france\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from criminals_with_city\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine all dataframes into one\n",
    "df_crime_type_profit = unionAll(list(dfs_dict.values()))\n",
    "print(list(df_crime_type_profit.columns))\n",
    "df_crime_type_profit.show(4, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crime_type_profit.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profit is a string - which is incorrect -> the schema needs to be redifined\n",
    "# or the column recasted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crime_type_profit = df_crime_type_profit.withColumn('profit', F.col('profit').cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crime_type_profit.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crime_type_profit.cache()\n",
    "df_crime_type_profit.coalesce(1).write.orc(\"./sql_data/crime_profit\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"drop table if exists crime_profit\")\n",
    "\n",
    "spark.sql(\"\"\"CREATE EXTERNAL TABLE crime_profit (\n",
    "  name string,\n",
    "  crime_type string,\n",
    "  profit int,\n",
    "  country string)\n",
    "STORED AS ORC\n",
    "LOCATION '/Users/vk/Documents/Python/holmes_moriarty_sql/src/sql_data/crime_profit'\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from crime_profit\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"select  a.id, a.name, a.alias, b.crime_type, b.profit, b.country\n",
    "            from criminals a\n",
    "            left join crime_profit b\n",
    "                on a.name = b.name and a.country = b.country\"\"\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"select count(*) as row_count from (\n",
    "                select  a.id, a.name, a.alias, b.crime_type, b.profit, b.country\n",
    "                from criminals a\n",
    "                left join crime_profit b\n",
    "                    on a.name = b.name and a.country = b.country)\"\"\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order by profit (descending) and \n",
    "# cast profit as int (to keep the same with pyspark notebook; not necessary here)\n",
    "spark.sql(\"\"\"select  a.id, a.name, a.alias, b.crime_type, CAST(b.profit AS INT), b.country\n",
    "            from criminals a\n",
    "            left join crime_profit b\n",
    "                on a.name = b.name and a.country = b.country\n",
    "            order by profit DESC\"\"\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the most profitable crime type\n",
    "\n",
    "spark.sql(\"\"\"select crime_type, sum(profit) as total_profit\n",
    "            from criminals a\n",
    "            left join crime_profit b\n",
    "                on a.name = b.name and a.country = b.country\n",
    "            group by crime_type\n",
    "            order by total_profit DESC\"\"\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark.sql(\"show tables\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the country where with the highest weapons sales\n",
    "\n",
    "spark.sql(\"\"\"select a.country, sum(profit) as total_profit\n",
    "            from criminals a\n",
    "            left join crime_profit b\n",
    "                on a.name = b.name and a.country = b.country\n",
    "            where crime_type = 'weapons sale'\n",
    "            group by a.country\n",
    "            order by total_profit DESC\n",
    "            \"\"\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city_profit.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top 5 salesmen in the selected country\n",
    "df_large_sales_alias_null = df_city_profit.where(\"country = '{}' and alias = '' and crime_type = '{}'\".format(top_country, crime_type_big_sales))\\\n",
    "                                            .orderBy(\"profit\", ascending = False)\n",
    "\n",
    "df_large_sales_alias_null.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 3\n",
    "\n",
    "Add date (last deal date) Moriarty does not deal on Sundays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_dates = spark.read.csv(\"./data/id_dates.csv\", header=True, inferSchema=True)\n",
    "print(\"id_dates shape: {}\".format(id_dates.count()))\n",
    "id_dates.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected_with_dates = df_city_profit.join(id_dates, on=[\"id\", \"country\"], how=\"left\")\n",
    "print(df_selected_with_dates.count())\n",
    "df_selected_with_dates.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected_with_dates.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DateType, StringType\n",
    "\n",
    "\n",
    "def weekday(date):\n",
    "    \"\"\" Generate day of the week based on date (as string or as datetime object)\"\"\"\n",
    "    \n",
    "    if isinstance(date, str):\n",
    "        from datetime import datetime\n",
    "        \n",
    "        date = datetime.strptime(date, \"%Y-%m-%d\")  # change the format if necessary\n",
    "        \n",
    "    return date.strftime(\"%A\")\n",
    "\n",
    "\n",
    "weekday_udf = udf(weekday, StringType())\n",
    "\n",
    "# conversion to DateType is not necessary as it is handled inside the function\n",
    "# here it is offered as an example of re-casting\n",
    "df_selected_with_dates = df_selected_with_dates.withColumn(\"date\", F.col(\"date\").cast(DateType()))\n",
    "\n",
    "df_selected_with_dates = df_selected_with_dates.withColumn(\"weekdate\", weekday_udf(\"date\").alias(\"weekday\"))\n",
    "df_selected_with_dates.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_type_big_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top 5 salesmen in the selected country\n",
    "df_final = df_selected_with_dates.where(\"\"\"country = '{}' \n",
    "                                            and alias = '' \n",
    "                                            and crime_type = '{}'\n",
    "                                            and weekday != 'Sunday'\n",
    "                                       \"\"\".format(top_country, crime_type_big_sales))\n",
    "\n",
    "df_final.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moriarty_name =  df_final.select(\"name\").collect()[0][0]\n",
    "print(\"The name Moriarty is hiding behind: {}\".format(moriarty_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
