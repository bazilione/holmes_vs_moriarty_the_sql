{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE GOAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to take Watson's role and using the intel (the data in the supplied files) from the police, Interpol, and undercover agents about Europe's criminals to identify the name behind which Moriarty is hiding. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1\n",
    "-Watson, just like our grand-grand-fathers we are again after Moriarty. \n",
    "\n",
    "We need to catch him. H-mmm... I need to be careful here - maybe it is not him, maybe it is her. All we know is \n",
    "that someone is masterminding unlawful activities and planning something bad. The Interpol agents, with the help of my boys, collected information that should provide us the clues to determine the name Moriarty's is hiding brhind and arrest him.\n",
    "\n",
    "-I have a number of .csv and .txt files about criminal activity and high-profile suspicious sales that were sent over from our neighbors: France, Germany, Netherlands, and our own MI-6 in the United Kingdom.\n",
    "\n",
    "So, the first task would be to combine the data into one table. I requested info on the name, alias, and the location of the last known whereabouts, as latitude and longitude, but since the data comes from all around the Europe they might have named the columns differently.\n",
    "\n",
    "I am thinking that adding the country to the data might be helpful in our future analysis.\n",
    "\n",
    "Lastly, from my correspondence with our undercover agents, all the activity seems to be happening around major financial centers. If the city names are not in the data, I suppose you can extract it based on the latitude and logitude. Mmmm... And a map of course, unless your knowledge of Europe's geography is excepitonal. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Text:\n",
    "Tasks:\n",
    "1. Read in data from the files into a separate dataframe and add the country name ('country' column).\n",
    "2. Identify the city around which the criminals operate. Add it to the dataframe ('city' column).\n",
    "3. Concatenate dfs into a single dataframe with the four original columns renamed to: [name, alias, latitude, longitude]\n",
    "4. Fill NAs in aliases with an empty string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0.1\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "print(pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# from https://datascience.stackexchange.com/questions/11356/merging-multiple-data-frames-row-wise-in-pyspark\n",
    "from functools import reduce  # For Python 3.x\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"hdfs://namenode/sql/metadata/hive\")\\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()\n",
    "# .config(\"spark.sql.warehouse.dir\", \"hdfs://namenode/sql/metadata/hive\") \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|default  |\n",
      "+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select database to use and create tables in\n",
    "spark.sql(\"show databases\").show(10, False)\n",
    "spark.sql(\"use default\")  # 'default' is a pre-created database where we can create tables\n",
    "# (we could have skipped this statement but it makes it more explicity which database we use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "|default |criminals|false      |\n",
      "|default |test     |false      |\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# we could also create our tables in our own custom database using:\n",
    "# spark.sql(\"create database moriarty_db\")\n",
    "# spark.sql(\"use moriarty_db\")\n",
    "spark.sql(\"show tables\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crime_type_profit_France.txt          criminals_Germany.csv\r\n",
      "crime_type_profit_Germany.txt         criminals_Netherlands.csv\r\n",
      "crime_type_profit_Netherlands.txt     criminals_United Kingdom.csv\r\n",
      "crime_type_profit_United Kingdom.txt  id_dates.csv\r\n",
      "criminals_France.csv\r\n"
     ]
    }
   ],
   "source": [
    "%ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country: United Kingdom, rows: 306\n",
      "Country: Germany, rows: 264\n",
      "Country: Netherlands, rows: 250\n",
      "Country: France, rows: 349\n",
      "Rows in combined df: 1169\n"
     ]
    }
   ],
   "source": [
    "# get the data\n",
    "\n",
    "def rename_cols(df, new_col_names):\n",
    "    \"\"\"\"\"\"\n",
    "    for col, new_col in zip(df.columns, new_col_names):\n",
    "        df = df.withColumnRenamed(col, new_col)\n",
    "        \n",
    "    return df\n",
    "\n",
    "#explore the dataframes: column names, shapes and combine into a single dataframe\n",
    "country_list = [\"United Kingdom\", \"Germany\", \"Netherlands\", \"France\"]\n",
    "dfs_dict = {}\n",
    "for country_ in country_list:\n",
    "    file_name = \"./data/criminals_{}.csv\".format(country_)\n",
    "    df = spark.read.csv(file_name, header=True, inferSchema=True)\n",
    "    print(\"Country: {}, rows: {}\".format(country_, df.count()))\n",
    "    new_col_names = [\"id\", \"name\", \"alias\", \"latitude\", \"longitude\"]\n",
    "    df = rename_cols(df, new_col_names)\n",
    "    df = df.withColumn('country', F.lit(country_))\n",
    "    dfs_dict[country_] = df  # add data frame to the dict for a future union\n",
    "# print(\"Len dfs_dict: {}\".format(len(dfs_dict)))\n",
    "\n",
    "\n",
    "\n",
    "def unionAll(dfs):\n",
    "    return reduce(DataFrame.unionAll, dfs)\n",
    "\n",
    "df_criminals_combined = unionAll(list(dfs_dict.values()))\n",
    "print(\"Rows in combined df: {}\".format(df_criminals_combined.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as ORC file (a popular data format in big data management)\n",
    "df_criminals_combined.cache()\n",
    "df_criminals_combined.coalesce(1).write.orc(\"./sql_data/criminals\", mode='overwrite') # doesn't have 'orc' extension as it is a folder\n",
    "# the file with extension '.orc' will be inside it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- alias: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- country: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check schema before casting and saving\n",
    "df_criminals_combined.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- alias: string (nullable = true)\n",
      " |-- latitude: float (nullable = true)\n",
      " |-- longitude: float (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# often to insure that datatypes are compatible for retrieval via Hive we need to explicitly define data types\n",
    "\n",
    "# import data types to cast data and define schema\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, \\\n",
    "                                DecimalType, FloatType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"alias\", StringType(), True),\n",
    "    StructField(\"latitude\", FloatType(), True),\n",
    "    StructField(\"longitude\", FloatType(), True),\n",
    "    StructField(\"country\", StringType(), True)\n",
    "])\n",
    "\n",
    "# apply schema\n",
    "df_criminals_combined_new_schema = spark.createDataFrame(df_criminals_combined.rdd, schema=schema)\n",
    "df_criminals_combined_new_schema.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a table that defines the data (including the data location)\n",
    "\n",
    "# drop table if we created it before and define the table by registering 'table definition'\n",
    "spark.sql(\"drop table if exists criminals\")\n",
    "\n",
    "# the hive datatypes should be appropriate (not necessarily identically named to spark datatypes)\n",
    "\n",
    "# 'EXTERNAL' (also could be 'external') makes sure that if the table is dropped (deleted) the data remains\n",
    "spark.sql(\"\"\"CREATE EXTERNAL TABLE criminals (\n",
    " id int,\n",
    " name string,\n",
    " alias string,\n",
    " latitude float,\n",
    " longitude float,\n",
    " country string\n",
    "    )\n",
    "STORED AS ORC\n",
    "LOCATION '/Users/vk/Documents/Python/holmes_moriarty_sql/src/sql_data/criminals'\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "| default|criminals|      false|\n",
      "| default|     test|      false|\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------+-----+--------+---------+--------------+\n",
      "|id |name                    |alias|latitude|longitude|country       |\n",
      "+---+------------------------+-----+--------+---------+--------------+\n",
      "|0  |Ms. Diane Barnett       |null |51.3327 |-0.0328  |United Kingdom|\n",
      "|1  |Elizabeth McDonald      |null |51.3732 |-0.0396  |United Kingdom|\n",
      "|2  |Jacqueline Martin-Winter|null |51.3536 |-0.223   |United Kingdom|\n",
      "|3  |Roger Farmer            |null |51.2891 |-0.208   |United Kingdom|\n",
      "|4  |Mrs. Georgina Harrison  |null |51.6004 |0.0054   |United Kingdom|\n",
      "|5  |Peter Stevens           |null |51.6441 |0.0188   |United Kingdom|\n",
      "|6  |Georgina Bell           |null |51.5304 |-0.0927  |United Kingdom|\n",
      "|7  |Miss Lesley Sullivan    |null |51.7303 |-0.2607  |United Kingdom|\n",
      "|8  |Keith Kelly             |Happy|51.4393 |-0.1421  |United Kingdom|\n",
      "|9  |Shane Bailey            |null |51.2735 |-0.3407  |United Kingdom|\n",
      "+---+------------------------+-----+--------+---------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "criminals_df = spark.sql(\"select * from criminals\")\n",
    "# print(\"Table (read-in) count: {}\".format(criminals_df.count()))\n",
    "criminals_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- alias: string (nullable = true)\n",
      " |-- latitude: float (nullable = true)\n",
      " |-- longitude: float (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "criminals_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+--------------------+\n",
      "|country       |avg_lat           |avg_lon             |\n",
      "+--------------+------------------+--------------------+\n",
      "|France        |48.86060943166301 |2.364566761989648   |\n",
      "|Germany       |50.097135254831024|8.678964380061988   |\n",
      "|Netherlands   |52.37530560302734 |4.900951610565185   |\n",
      "|United Kingdom|51.50456336276983 |-0.12400588218790493|\n",
      "+--------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculate mean latitude and longitude to identify the major financial centers (cities)\n",
    "# (copy and paste the lat, lon values into Google Maps)\n",
    "# dataframe.filter(df['salary'] > 100000).agg({\"age\": \"avg\"})\n",
    "\n",
    "spark.sql(\"\"\"select country, AVG(latitude) as avg_lat, AVG(longitude) as avg_lon\n",
    "                            from criminals\n",
    "                             group by country\n",
    "                             order by country\"\"\").show(10, False)\n",
    "# # print(\"Table (read-in) count: {}\".format(criminals_df.count()))\n",
    "# criminals_df.show(10, False)\n",
    "\n",
    "# for country_ in country_list:\n",
    "#     country_df = df_criminals_combined.where(\"country = '{}'\".format(country_))\n",
    "#     lat = round(country_df.agg({\"latitude\": \"avg\"}).collect()[0][0], 4)\n",
    "#     lon = round(country_df.agg({\"longitude\": \"avg\"}).collect()[0][0], 4)\n",
    "#     print(\"Country: {}, (lat, lon): {}, {}\".format(country_, lat, lon))\n",
    "#     print(40 * \"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+-------+\n",
      "|country       |avg_lat|avg_lon|\n",
      "+--------------+-------+-------+\n",
      "|France        |48.8606|2.3646 |\n",
      "|Germany       |50.0971|8.679  |\n",
      "|Netherlands   |52.3753|4.901  |\n",
      "|United Kingdom|51.5046|-0.124 |\n",
      "+--------------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select country,\n",
    "                    ROUND(AVG(latitude), 4) as avg_lat,\n",
    "                    ROUND(AVG(longitude), 4) as avg_lon\n",
    "                from criminals\n",
    "                 group by country\n",
    "                 order by country\"\"\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the city name to the df\n",
    "\n",
    "#it can be done using a series of if/else statements, such as 'if country_ == 'France': city = 'Paris', etc. OR\n",
    "# using a dictionary as below:\n",
    "country_city_dict = {\"United Kingdom\": \"London\", \"Germany\": \"Frankfurt\", \"Netherlands\": \"Amsterdam\", \"France\": \"Paris\"}\n",
    "country_city_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------+-----+--------+---------+--------------+------+\n",
      "|id |name                    |alias|latitude|longitude|country       |city  |\n",
      "+---+------------------------+-----+--------+---------+--------------+------+\n",
      "|0  |Ms. Diane Barnett       |null |51.3327 |-0.0328  |United Kingdom|London|\n",
      "|1  |Elizabeth McDonald      |null |51.3732 |-0.0396  |United Kingdom|London|\n",
      "|2  |Jacqueline Martin-Winter|null |51.3536 |-0.223   |United Kingdom|London|\n",
      "|3  |Roger Farmer            |null |51.2891 |-0.208   |United Kingdom|London|\n",
      "|4  |Mrs. Georgina Harrison  |null |51.6004 |0.0054   |United Kingdom|London|\n",
      "|5  |Peter Stevens           |null |51.6441 |0.0188   |United Kingdom|London|\n",
      "|6  |Georgina Bell           |null |51.5304 |-0.0927  |United Kingdom|London|\n",
      "|7  |Miss Lesley Sullivan    |null |51.7303 |-0.2607  |United Kingdom|London|\n",
      "|8  |Keith Kelly             |Happy|51.4393 |-0.1421  |United Kingdom|London|\n",
      "|9  |Shane Bailey            |null |51.2735 |-0.3407  |United Kingdom|London|\n",
      "+---+------------------------+-----+--------+---------+--------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select *, \n",
    "                case \n",
    "                    when country = 'United Kingdom' then 'London'\n",
    "                    when country = 'France' then 'Paris'\n",
    "                    when country = 'Germany' then 'Frankfurt'\n",
    "                    when country = 'Netherlands' then 'Amsterdam'\n",
    "                end as city\n",
    "            from criminals\"\"\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------+-----+--------------+------+\n",
      "|id |name                    |alias|country       |city  |\n",
      "+---+------------------------+-----+--------------+------+\n",
      "|0  |Ms. Diane Barnett       |     |United Kingdom|London|\n",
      "|1  |Elizabeth McDonald      |     |United Kingdom|London|\n",
      "|2  |Jacqueline Martin-Winter|     |United Kingdom|London|\n",
      "|3  |Roger Farmer            |     |United Kingdom|London|\n",
      "|4  |Mrs. Georgina Harrison  |     |United Kingdom|London|\n",
      "|5  |Peter Stevens           |     |United Kingdom|London|\n",
      "|6  |Georgina Bell           |     |United Kingdom|London|\n",
      "|7  |Miss Lesley Sullivan    |     |United Kingdom|London|\n",
      "|8  |Keith Kelly             |Happy|United Kingdom|London|\n",
      "|9  |Shane Bailey            |     |United Kingdom|London|\n",
      "+---+------------------------+-----+--------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fill Nas with empty string \n",
    "# we'll also assign this new data to a variable name for saving and creating a new table to use later\n",
    "# (note that 'show' method is moved to the spark dataframe)\n",
    "criminals_with_city = spark.sql(\"\"\"select id, name,\n",
    "                case\n",
    "                    when alias is null then ''\n",
    "                    else alias\n",
    "                end as alias,\n",
    "                country,\n",
    "                case \n",
    "                    when country = 'United Kingdom' then 'London'\n",
    "                    when country = 'France' then 'Paris'\n",
    "                    when country = 'Germany' then 'Frankfurt'\n",
    "                    when country = 'Netherlands' then 'Amsterdam'\n",
    "                end as city\n",
    "            from criminals\"\"\")\n",
    "\n",
    "criminals_with_city.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- alias: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "criminals_with_city.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "criminals_with_city.cache()\n",
    "criminals_with_city.coalesce(1).write.orc(\"./sql_data/criminals_with_city\", mode='overwrite') # doesn't have 'orc' extension as it is a folder\n",
    "# the file with extension '.orc' will be inside it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"drop table if exists criminals_with_city\")\n",
    "\n",
    "spark.sql(\"\"\"CREATE EXTERNAL TABLE criminals_with_city (\n",
    " id int,\n",
    " name string,\n",
    " alias string,\n",
    " country string,\n",
    " city string)\n",
    "STORED AS ORC\n",
    "LOCATION '/Users/vk/Documents/Python/holmes_moriarty_sql/src/sql_data/criminals_with_city'\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------+-----+--------------+------+\n",
      "|id |name                    |alias|country       |city  |\n",
      "+---+------------------------+-----+--------------+------+\n",
      "|0  |Ms. Diane Barnett       |     |United Kingdom|London|\n",
      "|1  |Elizabeth McDonald      |     |United Kingdom|London|\n",
      "|2  |Jacqueline Martin-Winter|     |United Kingdom|London|\n",
      "|3  |Roger Farmer            |     |United Kingdom|London|\n",
      "|4  |Mrs. Georgina Harrison  |     |United Kingdom|London|\n",
      "|5  |Peter Stevens           |     |United Kingdom|London|\n",
      "|6  |Georgina Bell           |     |United Kingdom|London|\n",
      "|7  |Miss Lesley Sullivan    |     |United Kingdom|London|\n",
      "|8  |Keith Kelly             |Happy|United Kingdom|London|\n",
      "|9  |Shane Bailey            |     |United Kingdom|London|\n",
      "+---+------------------------+-----+--------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check that the data is readable\n",
    "spark.sql(\"select * from criminals_with_city\").show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "Add crime_type and profit info to criminals. \n",
    "#(merge/join) criminals table with the crime type and profit information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Great, Watson! \n",
    "- Now we need to know what everyone of those supspects did wrong, that is the crime type, and desirably, how much they profited from it: Moriarty is not a small fish. He is in the category with th largest total sales.\n",
    "\n",
    "- You'll need to add the crime type and the profit from the files to the table you already put together. Be mindful of the file types. I also believe that the separator in these file maybe different from the files you used previously.\n",
    "-Moriarty made one of the top 5 sales last year. He is not stupid for nicknames, I am pretty sure he doesn't have an alias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution (task 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows: 306\n",
      "rows: 264\n",
      "rows: 250\n",
      "rows: 349\n",
      "Len dfs_dict: 4\n",
      "['name', 'crime_type', 'profit', 'country']\n",
      "+------------------------+----------+------+--------------+\n",
      "|name                    |crime_type|profit|country       |\n",
      "+------------------------+----------+------+--------------+\n",
      "|Ms. Diane Barnett       |theft     |284   |United Kingdom|\n",
      "|Elizabeth McDonald      |theft     |59    |United Kingdom|\n",
      "|Jacqueline Martin-Winter|forgery   |150   |United Kingdom|\n",
      "|Roger Farmer            |theft     |378   |United Kingdom|\n",
      "+------------------------+----------+------+--------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# union(concatenate) files for the latest crime dates\n",
    "\n",
    "country_list = [\"United Kingdom\", \"Germany\", \"Netherlands\", \"France\"]\n",
    "dfs_dict = {}\n",
    "for country_ in country_list:\n",
    "    file_name = \"./data/crime_type_profit_{}.txt\".format(country_)\n",
    "    df = spark.read.csv(file_name, header=True, sep=\" \")\n",
    "    print(\"rows: {}\".format(df.count()))\n",
    "    df = df.withColumn('country', F.lit(country_))\n",
    "    dfs_dict[country_] = df\n",
    "print(\"Len dfs_dict: {}\".format(len(dfs_dict)))\n",
    "\n",
    "#combine all dataframes into one\n",
    "df_crime_type_profit = unionAll(list(dfs_dict.values()))\n",
    "print(list(df_crime_type_profit.columns))\n",
    "df_crime_type_profit.show(4, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- crime_type: string (nullable = true)\n",
      " |-- profit: string (nullable = true)\n",
      " |-- country: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_crime_type_profit.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profit is a string - which is incorrect -> the schema needs to be redifined\n",
    "# or the column recasted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crime_type_profit = df_crime_type_profit.withColumn('profit', F.col('profit').cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- crime_type: string (nullable = true)\n",
      " |-- profit: integer (nullable = true)\n",
      " |-- country: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_crime_type_profit.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crime_type_profit.cache()\n",
    "df_crime_type_profit.coalesce(1).write.orc(\"./sql_data/crime_profit\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"drop table if exists crime_profit\")\n",
    "\n",
    "spark.sql(\"\"\"CREATE EXTERNAL TABLE crime_profit (\n",
    "  name string,\n",
    "  crime_type string,\n",
    "  profit int,\n",
    "  country string)\n",
    "STORED AS ORC\n",
    "LOCATION '/Users/vk/Documents/Python/holmes_moriarty_sql/src/sql_data/crime_profit'\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+----------+------+--------------+\n",
      "|name                    |crime_type|profit|country       |\n",
      "+------------------------+----------+------+--------------+\n",
      "|Ms. Diane Barnett       |theft     |284   |United Kingdom|\n",
      "|Elizabeth McDonald      |theft     |59    |United Kingdom|\n",
      "|Jacqueline Martin-Winter|forgery   |150   |United Kingdom|\n",
      "|Roger Farmer            |theft     |378   |United Kingdom|\n",
      "|Mrs. Georgina Harrison  |theft     |55    |United Kingdom|\n",
      "|Peter Stevens           |robbery   |868   |United Kingdom|\n",
      "|Georgina Bell           |theft     |365   |United Kingdom|\n",
      "|Miss Lesley Sullivan    |forgery   |320   |United Kingdom|\n",
      "|Keith Kelly             |theft     |399   |United Kingdom|\n",
      "|Shane Bailey            |forgery   |495   |United Kingdom|\n",
      "+------------------------+----------+------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from crime_profit\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------+-----+--------+---------+--------------+------------------------+----------+------+--------------+\n",
      "|id |name                    |alias|latitude|longitude|country       |name                    |crime_type|profit|country       |\n",
      "+---+------------------------+-----+--------+---------+--------------+------------------------+----------+------+--------------+\n",
      "|0  |Ms. Diane Barnett       |null |51.3327 |-0.0328  |United Kingdom|Ms. Diane Barnett       |theft     |284   |United Kingdom|\n",
      "|1  |Elizabeth McDonald      |null |51.3732 |-0.0396  |United Kingdom|Elizabeth McDonald      |theft     |59    |United Kingdom|\n",
      "|2  |Jacqueline Martin-Winter|null |51.3536 |-0.223   |United Kingdom|Jacqueline Martin-Winter|forgery   |150   |United Kingdom|\n",
      "|3  |Roger Farmer            |null |51.2891 |-0.208   |United Kingdom|Roger Farmer            |theft     |378   |United Kingdom|\n",
      "|4  |Mrs. Georgina Harrison  |null |51.6004 |0.0054   |United Kingdom|Mrs. Georgina Harrison  |theft     |55    |United Kingdom|\n",
      "|5  |Peter Stevens           |null |51.6441 |0.0188   |United Kingdom|Peter Stevens           |robbery   |868   |United Kingdom|\n",
      "|6  |Georgina Bell           |null |51.5304 |-0.0927  |United Kingdom|Georgina Bell           |theft     |365   |United Kingdom|\n",
      "|7  |Miss Lesley Sullivan    |null |51.7303 |-0.2607  |United Kingdom|Miss Lesley Sullivan    |forgery   |320   |United Kingdom|\n",
      "|8  |Keith Kelly             |Happy|51.4393 |-0.1421  |United Kingdom|Keith Kelly             |theft     |399   |United Kingdom|\n",
      "|9  |Shane Bailey            |null |51.2735 |-0.3407  |United Kingdom|Shane Bailey            |forgery   |495   |United Kingdom|\n",
      "+---+------------------------+-----+--------+---------+--------------+------------------------+----------+------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select * from criminals a\n",
    "          left join crime_profit b\n",
    "          on a.name = b.name and a.country = b.country\"\"\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# drop duplicates \n",
    "df_with_city = df_with_city.drop_duplicates([\"name\"])\n",
    "df_with_city.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join main criminal info with crime type and profit\n",
    "df_city_profit = df_with_city.join(df_crime_type_profit, [\"name\",\"country\"], \"left\")\n",
    "print(\"Df shape: {}\".format(df_city_profit.count()))\n",
    "# print(df_city_profit.columns)./\n",
    "df_city_profit.orderBy('profit', ascending = False).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profit column is not sorted properly. possibly the data type is the issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city_profit.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city_profit = df_city_profit.withColumn(\"profit\", F.col(\"profit\").cast(\"int\"))\n",
    "df_city_profit.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's order by profit again...\n",
    "df_city_profit.orderBy('profit', ascending = False).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#investigate crime types and get total sales for each\n",
    "df_by_profit = df_city_profit.groupBy([\"crime_type\"]).\\\n",
    "                agg(F.sum(\"profit\").alias(\"total_profit\")).\\\n",
    "                orderBy(\"total_profit\", ascending=False)\n",
    "\n",
    "df_by_profit.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_type_big_sales = df_by_profit.select(\"crime_type\").collect()[0][0]\n",
    "crime_type_big_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"crime_type = '{}'\".format(crime_type_big_sales))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city_profit.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_crime_type_profit_df = df_city_profit.where(\"crime_type == '{}'\".format(crime_type_big_sales))\\\n",
    "                    .groupBy([\"country\"])\\\n",
    "                    .agg(F.sum(\"profit\").alias('total_profit'))\\\n",
    "                    .orderBy(\"total_profit\", ascending=False)\n",
    "    \n",
    "countries_crime_type_profit_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_country = countries_crime_type_profit_df.select(\"country\").collect()[0][0]\n",
    "top_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city_profit.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top 5 salesmen in the selected country\n",
    "df_large_sales_alias_null = df_city_profit.where(\"country = '{}' and alias = '' and crime_type = '{}'\".format(top_country, crime_type_big_sales))\\\n",
    "                                            .orderBy(\"profit\", ascending = False)\n",
    "\n",
    "df_large_sales_alias_null.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 3\n",
    "\n",
    "Add date (last deal date) Moriarty does not deal on Sundays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_dates = spark.read.csv(\"./data/id_dates.csv\", header=True, inferSchema=True)\n",
    "print(\"id_dates shape: {}\".format(id_dates.count()))\n",
    "id_dates.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected_with_dates = df_city_profit.join(id_dates, on=[\"id\", \"country\"], how=\"left\")\n",
    "print(df_selected_with_dates.count())\n",
    "df_selected_with_dates.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected_with_dates.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DateType, StringType\n",
    "\n",
    "\n",
    "def weekday(date):\n",
    "    \"\"\" Generate day of the week based on date (as string or as datetime object)\"\"\"\n",
    "    \n",
    "    if isinstance(date, str):\n",
    "        from datetime import datetime\n",
    "        \n",
    "        date = datetime.strptime(date, \"%Y-%m-%d\")  # change the format if necessary\n",
    "        \n",
    "    return date.strftime(\"%A\")\n",
    "\n",
    "\n",
    "weekday_udf = udf(weekday, StringType())\n",
    "\n",
    "# conversion to DateType is not necessary as it is handled inside the function\n",
    "# here it is offered as an example of re-casting\n",
    "df_selected_with_dates = df_selected_with_dates.withColumn(\"date\", F.col(\"date\").cast(DateType()))\n",
    "\n",
    "df_selected_with_dates = df_selected_with_dates.withColumn(\"weekdate\", weekday_udf(\"date\").alias(\"weekday\"))\n",
    "df_selected_with_dates.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_type_big_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top 5 salesmen in the selected country\n",
    "df_final = df_selected_with_dates.where(\"\"\"country = '{}' \n",
    "                                            and alias = '' \n",
    "                                            and crime_type = '{}'\n",
    "                                            and weekday != 'Sunday'\n",
    "                                       \"\"\".format(top_country, crime_type_big_sales))\n",
    "\n",
    "df_final.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moriarty_name =  df_final.select(\"name\").collect()[0][0]\n",
    "print(\"The name Moriarty is hiding behind: {}\".format(moriarty_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
